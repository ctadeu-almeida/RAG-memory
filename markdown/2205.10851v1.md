IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

1

Vision-based Anti-UAV Detection and Tracking

Jie Zhao, Jingshu Zhang, Dongdong Li, Dong Wang

2
2
0
2

y
a
M
2
2

]

V
C
.
s
c
[

1
v
1
5
8
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Unmanned aerial vehicles (UAV) have been widely
used in various ﬁelds, and their invasion of security and privacy
has aroused social concern. Several detection and tracking
systems for UAVs have been introduced in recent years, but most
of them are based on radio frequency, radar, and other media.
We assume that the ﬁeld of computer vision is mature enough
to detect and track invading UAVs. Thus we propose a visible
light mode dataset called Dalian University of Technology Anti-
UAV dataset, DUT Anti-UAV for short. It contains a detection
dataset with a total of 10,000 images and a tracking dataset with
20 videos that include short-term and long-term sequences. All
frames and images are manually annotated precisely. We use
this dataset to train several existing detection algorithms and
evaluate the algorithms’ performance. Several tracking methods
are also tested on our tracking dataset. Furthermore, we propose
a clear and simple tracking algorithm combined with detection
that inherits the detector’s high precision. Extensive experiments
show that the tracking performance is improved considerably
after fusing detection, thus providing a new attempt at UAV
tracking using our dataset. The datasets and results are publicly
available at: https://github.com/wangdongdut/DUT-Anti-UAV.

Index Terms—Anti-UAV, dataset, detection, tracking.

I. INTRODUCTION

W ITH the maturity of industrial technology, unmanned

aerial vehicles (UAV) have gradually become main-
transporta-
stream. They are widely used in logistics [1],
tion [2], monitoring [3], and other ﬁelds because of their
small size, low price, and simple operation [4]. Although
UAVs provide convenience, they cause a series of problems.
Either public safety or personal safety and privacy are easily
violated. Therefore, the detection and tracking of illegally
or unintentionally invading UAVs are crucial. However, no
complete and reliable anti-UAV detection and tracking system
is available at present. Most of the existing detection and
early warning technologies are based on radar [5], radio
frequency (RF) [6], and acoustic sensors [7], which often have
limitations, such as high cost and susceptibility to noise. These
limitations lead to unreliable results. Therefore, these existing
algorithms cannot be used extensively. Their application range
is limited to airports and other public places.

In recent years, methods based on deep learning have
developed rapidly in various ﬁelds [8], [9], [10], [11], [12]
of computer vision, especially for object detection and track-
ing. Their maturity provides the possibility of establishing a
high-performance tracking system of anti-UAV. Many generic
object detection models, such as Faster-RCNN [11] and
SSD [13], and common tracking models, such as SiamFC [12]
and DiMP [14], are currently available. However, these generic
methods do not perform well when directly applied to UAV
detection and tracking. Even though detection algorithms have
gradually become mature and commercialized, small-target
detection in the complex background is still a problem, which

anti-UAV detection aims to address. UAV often fuses with the
complex background with much noise and interference. Occlu-
sion also occurs and brings challenges to the tracking task. A
series of methods, such as improving YOLOv3 [15], which
uses low-rank and sparse matrix decomposition to conduct
classiﬁcation [16], are proposed to solve the aforementioned
problems, and achieve good results.

The main motivation of our work is to use existing state-of-
the-art detection and tracking methods to effectively adapt and
address the anti-UAV task in the data level and method level.
First, deep learning-based methods require plenty of training
data to obtain robust and accurate performance. Although
several corresponding datasets are proposed, such as Anti-
UAV [17] and MAV-VID [18], they are still not enough to
train a high-performance model. Therefore, to make full use
of existing detection and tracking methods for the anti-UAV
task in the data level, and promote further development of this
area, we propose a visible light dataset for UAVs, including
detection and tracking subsets. We also retrain several detec-
tion methods using our training set. Second, we attempt to
further improve the UAV tracking performance at the method
level. To be speciﬁc, we propose a fusion strategy to combine
detection and tracking methods.

Our main contributions are summarized as follows.
• We propose an anti-UAV dataset called DUT Anti-UAV
that contains detection and tracking subsets. The detection
dataset includes a training set (5200 images), a validation
set (2600 images), and a testing set (2200 images). The
tracking dataset includes 20 sequences. It will be released
publicly for academic research.

• We evaluate state-of-the-art methods on our dataset, in-
cluding 14 detectors and 8 trackers. Detectors are all
retrained using the training set of our DUT Anti-UAV
detection dataset.

• A clear and simple fusion algorithm is proposed for the
UAV tracking task. This algorithm integrates detection
into tracking while taking the advantage of the detec-
tor’s high precision. Extensive experiments show that the
tracking performance is improved signiﬁcantly for most
combinations of trackers and detectors.

II. RELATED WORK

A. Object detection and tracking under the UAV view

Different from Anti-UAV tasks, nowadays there are more
discussions about object detection and tracking from the UAV
view. Compared to cameras on moving vehicles, a UAV is
more ﬂexible for it is easy to control. Therefore, UAV is often
used to realize aerial object tracking. Several UAV datasets
have been constructed so far, e.g., UAV123 [19] for tracking,
DroneSURF [20] and CARPK [21] for detection, and so on.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

2

Besides, several corresponding algorithms [22], [23], [24] have
been proposed to address these two tasks. UAV detection
and tracking are mostly overlooking from above, for which
it gains large view scope. However, it brings new challenges,
such as high density, small object and complex background.
For these properties, Yu et al. [22] consider contextual infor-
mation using the Exchange Object Context Sampling(EOCS)
method [25] in tracking, to infer the relationships between
the objects. To solve the problem of fast camera motion, Li
et al. [23] optimize the camera motion model by projective
transformation based on background feature points. Besides,
Xing et al. [24] consider that in real time tracking, computing
resources employed on UAVs are limited. To complement
lightweight network, they propose a lightweight Transformer
layer, and then integrate it into pyramid networks, thus ﬁnally
build a real-time CPU-based tracker.

Aforementioned algorithms perform well on existing UAV
tracking benchmarks, as well as promote the commercializa-
tion of aerial object tracking. UAV tracking is more and more
popular and draw increasing attention, which makes the anti-
UAV tracking essential as well.

B. Anti-UAV methodology

Safety issues derived from UAVs have elicited increasing in
recent years. In particular, considering national security, many
countries have invested much time and energy in researching
and deploying quite mature anti-unmanned systems that are
not based on deep learning in military bases. Universities and
research institutions are continuously optimizing these anti-
unmanned systems.

ADS-ZJU [26]. This system combines multiple surveillance
technologies to realize drone detection, localization, and de-
fense. It deploys three sensors to collect acoustic signals, video
images, and RF signals. The information is then sent to the
central processing unit to extract features for detection and
localization. ADS-ZJU uses a short-time Fourier transform to
extract spectrum features of the received acoustic signals, and
histograms of oriented gradients to describe the image feature.
It also takes advantage of the characteristic that the spectrum
of the UAV’s RF signal is different from that of the WiFi
signal by using the distribution of RF signals’ strengths at
different communication channels to describe the RF feature.
After feature extraction, it utilizes support vector machine
(SVM) to conduct audio detection, video detection, and RF
detection in parallel. After that, the location of UAVs can be
estimated via hybrid measurements, including DOA and RSS,
under the constraints of the speciﬁc geographical area from
video images. The use of multiple surveillance technologies
complements the advantages and disadvantages of multiple
technologies, so that the system has high accuracy. Meanwhile,
it can conduct radio frequency interference which simple
vision-based system cannot do. But in this system, each unit
is scattered, which makes the system covers a large area, and
its high cost also makes it not suitable for civil use.

Dynamic coordinate tracing [27]. This study proposes a
dual-axis rotary tracking mechanism, using a dual-axis tracing
device, namely, two sets of step motors with a thermal imaging

or full-color camera and sensing module to measure the
UAV’s ﬂight altitude. The device dynamically calculates the
longitude and latitude coordinates in spherical coordinates.
The thermal imaging and full-color cameras are optionally
used under various weather conditions, making the system
robust in different environments. This drone tracking device
for anti-UAV systems is inexpensive and practical, however,
its requirements for hardware facilities are still high.

C. UAV dataset

In addition to using other media to solve the problem
of UAV detection, people have also begun to utilize deep
learning based object tracking algorithms for UAV tracking
due to the rapid development of computer vision in recent
years. In the task of computer vision, dataset is an important
factor in obtaining a model with strong robustness. Therefore,
datasets for UAV detection and tracking have been proposed
consistently. Several relatively complete existing UAV datasets
are described below.

MAV-VID [18]. This is a dataset published by Kaggle in
which UAV is the only detected object. It contains 64 videos
(40,323 pictures in total), of which 53 are used for training and
11 are used for validation. In this dataset, the locations of the
UAVs are relatively concentrated, and the differences between
locations are mostly horizontal. The detected objects are small,
the average size of which is 0.66% of the entire picture. While
in our dataset, the distribution of UAV is scattered, and the
horizontal and vertical distribution are relatively more uniform,
which makes the model trained by our dataset more robust.

Drone-vs-Bird Detection Challenge [28]. This dataset
is proposed in the 16-th IEEE International Conference on
Advanced Video and Signal-based Surveillance (AVSS). As
the name indicates, the prime characteristic of this dataset is
that in addition to UAVs, a number of birds cannot be ignored
in the pictures. The detector must successfully distinguish the
drones and birds, alerting against UAV while not responding to
the birds. However, the size, color, and even shape of the two
may be similar, which brings challenges to the detection task.
Different from the ﬁrst version, this dataset adds land scenes
in addition to sea scenes, which are shot by different cameras.
Another characteristic of this dataset is that the size of the
detected object is extremely small. According to statistical
analysis, the average size of the detected UAVs is 34 × 23
(0.1% of the image size). Seventy-seven videos consist of
nearly 10,000 images. In light of this situation, improving the
algorithm with regard to this dataset, successfully reducing the
high false positive rates, and further popularizing the method
to other ﬁelds if it is robust make up the signiﬁcance of this
dataset. Scenes in such datasets are most seaside with a wide
visual ﬁeld. Different from them, we collect data mostly in
the place with lots of buildings, which is more suitable for
civilian use.

Anti-UAV [17]. This is a dataset labeled with visible and
infrared dual-mode information, which consists of 318 fully
labeled videos. One hundred and sixty of the videos are used
as the training set, 91 are used as the testing set, and the rest
are used as the validation set, with a total of 186,494 images.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

3

(a) Detection-train

(b) Detection-test

(c) Detection-val

(d) Tracking

Fig. 1. Position distribution of the DUT Anti-UAV dataset.

(a) Aspect ratio (Detection-train)

(b) Aspect ratio (Detection-test)

(c) Aspect ratio (Detection-val)

(d) Aspect ratio (Tracking)

(e) Scale (Detection-train)

(f) Scale (Detection-test)

(g) Scale (Detection-val)

(h) Scale (Tracking)

Fig. 2. Aspect ratio and scale distribution of the DUT Anti-UAV dataset.

The UAVs in the dataset are divided into seven attributes,
which systematically conclude several special circumstances
that might appear in UAV detection tasks. The recorded videos
contain two environments, namely, day and night. In the two
environments, the detection of the two modals plays different
roles. From the perspective of location distribution, the range
of motion of Anti-UAV is wide, but mostly concentrated
in the central area, and it has a smaller variance compared
with the two other datasets and our dataset. This dataset
focuses on solving the problem that vision-based detector
would show poor performance in night, while our dataset aims
to improve models’ robustness through enriching the diversity
of multiple aspects, such as different UAV types, diverse scene
information, various light conditions, and different weathers.

Brian et al. [29] collect and integrate the aforementioned
three UAV datasets (i.e., MAV-VID [18], Drone-vs-Bird [28],
and Anti-UAV [17]), and present a benchmark performance
study using state of the art four object detection (Faster-
RCNN [11], YOLOv3 [30], SSD [13], and DETR [31]) and
three tracking methods (SORT [32], DeepSORT [33], and
Tracktor [34]). Compared with this work, we propose a new
dataset for both UAV detection and tracking tasks. Besides,
our experiments are more sufﬁcient. We evaluate 14 different

versions of detectors from the combination of ﬁve types of
detectors and three types of backbone networks. We also
present the tracking performance of 8 various trackers on our
dataset.

There is also a challenge [35] in the Anti-UAV commu-
nity, which has been held twice until now. This challenge
encourages novel and accurate methods for multi-scale object
tracking, greatly promoting the development of this task.
For example, SiamSTA [36], the winner of the 2nd Anti-
UAV Challenge, proposes a spatial-temporal attention based
Siamese tracker, which poses spatial and temporal constraints
on generating candidate proposals with local neighborhoods.

III. DUT ANTI-UAV BENCHMARK

To assist in the development of the area of UAV detection
and tracking, we propose a UAV detection and tracking
dataset, named DUT Anti-UAV. It contains detection and
tracking subsets. The detection dataset is split into three sets,
namely, training, testing, and veriﬁcation sets. The tracking
dataset contains 20 sequences where the targets are various
UAVs. It is used to test the performance of algorithms for
UAV tracking.

0.00.20.40.60.81.0X0.00.20.40.60.81.0Y0.00.20.40.60.81.0X0.00.20.40.60.81.0Y0.00.20.40.60.81.0X0.00.20.40.60.81.0Y0.00.20.40.60.81.0X0.00.20.40.60.81.0Y12345object aspect ratio0255075100125150175Num12345object aspect ratio0102030405060Num123456object aspect ratio020406080Num1.01.52.02.53.03.54.04.5object aspect ratio05001000150020002500Num0.00.10.20.30.40.50.60.7object area ratio01000200030004000Num0.00.10.20.30.4object area ratio0200400600800100012001400Num0.00.10.20.30.40.50.60.7object area ratio0500100015002000Num0.000.010.020.030.04object area ratio0500100015002000250030003500NumIEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

4

TABLE I
ATTRIBUTE DETAILS OF THE DUT ANTI-UAV DATASET.

Detection

Tracking

Num

5243
2245
2621
10109
1050
83
100
341
450
200
2480
2305
2500
2635
1000
1485
1915
590
1350
1285
780
1320
1300
1635
24804

train
test
val
All
video01
video02
video03
video04
video05
video06
video07
video08
video09
video10
video11
video12
video13
video14
video15
video16
video17
video18
video19
video20
All

Image size

max
3744 × 5616
1080 × 1920
2848 × 4288
3744 × 5616

min
160 × 240
360 × 640
213 × 320
160 × 240

1080 × 1920
720 × 1280
720 × 1280
1080 × 1920
720 × 1280
1080 × 1920
720 × 1280
720 × 1280
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
1080 × 1920
-

Object area ratio
avg
0.013
0.014
0.013
0.013
0.0048
0.0017
0.0023
0.0032
0.0020
0.0044
0.012
0.0056
0.0041
0.0028
0.0068
0.0017
7.9e-04
0.0017
0.0021
8.0e-04
0.0010
0.0013
7.4e-04
0.0016
0.0031

min
2.6e-05
4.1e-05
1.9e-06
1.9e-06
0.0012
8.5e-04
6.0e-04
6.4e-04
8.6e-04
0.0011
0.0023
0.0011
0.0011
0.0015
0.0038
4.9e-04
3.3e-04
7.9e-04
5.2e-04
3.6e-04
4.9e-04
3.3e-04
2.7e-04
5.8e-04
2.7e-04

max
0.70
0.47
0.69
0.70
0.021
0.0047
0.012
0.011
0.0032
0.011
0.045
0.030
0.0099
0.0048
0.015
0.0056
0.0048
0.0044
0.0048
0.0014
0.0032
0.0048
0.0018
0.0035
0.045

Object aspect ratio
min
avg
max
1.00
1.91
5.42
1.00
1.92
5.09
1.00
1.91
6.67
1.00
1.91
6.67
1.64
2.10
2.59
2.71
2.98
3.62
1.00
1.08
1.18
1.00
1.36
2.28
1.04
1.34
1.50
1.14
1.28
1.61
1.00
2.12
4.00
1.00
2.32
3.98
1.00
1.80
3.82
1.00
2.26
4.33
1.00
1.87
2.47
1.00
1.64
2.41
1.00
1.76
2.92
1.24
1.76
2.64
1.00
1.21
1.94
1.00
1.29
2.29
1.00
1.43
2.55
1.00
1.54
2.47
1.00
1.50
2.52
1.00
1.75
2.76
1.00
1.72
4.33

image contains multiple objects, the total number of detection
objects is 10,109, where the training, testing, and validation
sets have 5243, 2245, and 2621 objects, respectively.

B. Dataset characteristics

Compared with general object detection and tracking
datasets (e.g., COCO [37],
ILSVRC [38], LaSOT [39],
OTB [40]), the most notable characteristic of the proposed
dataset for UAV detection and tracking is that the proportion
of small objects is larger. In addition, given that UAVs mostly
ﬂy outdoors, the background is usually complicated, which
increases the difﬁculty of UAV detection and tracking tasks.
We analyze the characteristics of the proposed dataset from
the following aspects.

Image resolution. The dataset contains images with various
resolutions. For the detection dataset, the height and width of
the largest image are 3744 and 5616, whereas the size of the
smallest image is 160 × 240; a huge difference between them.
The tracking dataset has two type frames with 1080×1920 and
720 × 1280 resolutions. Various settings of image resolution
can make models adapt to images with different sizes, and
avoid overﬁtting.

Object and background. To enrich the diversity of objects
and prevent models from overﬁtting, we select more than 35
types of UAVs. Several examples can be seen in Fig. 3. The
scene information in the dataset is also diverse. Given that
UAVs mostly ﬂy outdoors, the background of our dataset

Fig. 3. Examples of different types of UAVs in our dataset.

A. Dataset splitting

Our DUT Anti-UAV dataset contains detection and tracking
subsets. The detection dataset is split into training, testing,
and validation sets. The tracking dataset contains 20 short-
term and long-term sequences. All frames and images are
manually annotated precisely. The detailed information of
images and objects is shown in Table I. Speciﬁcally,
the
detection dataset contains 10,000 images in total, in which the
training, testing, and validation sets have 5200, 2200 and 2600
images, respectively. In consideration of the situation that one

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

5

Fig. 4. Examples of the detection images and annotations of our dataset.

is an outdoor environment, including the sky, dark clouds,
jungles, high-rise buildings, residential buildings, farmland,
and playgrounds. Besides, a variety of light conditions (such as
day, night, dawn and dusk), and different weathers (like sunny,
cloudy, and snowy day) are also considered in our dataset.
Various examples from the detection subset are shown in
Fig. 4. Complicated background and obvious outdoor lighting
changes in our dataset are crucial for training a robust and
high-performed UAV detection model.

Object scale. The sizes of UAVs are often small, and the
outdoor environment is broad. Thus the proportion of small
objects in our dataset is large. We calculate the object area
ratio based on the full image and plot the histogram of the
scale distribution, shown as Table I and Fig. 2, respectively.
For the detection dataset, including the training, testing, and
validation sets, the average object area ratio is approximately
0.013, the smallest object area ratio is 1.9e-06, and the largest
object accounts for 0.7 of the entire image. Most of the
objects are small. The proportions of the objects’ size in the
entire image are approximately less than 0.05. For the tracking
dataset, the scales of objects in the sequences change smoothly.
The average object area ratio is 0.0031, the maximum ratio
is 0.045, and the minimum ratio is 2.7e-04. Compared with
objects in general detection and tracking datasets, small objects
are much harder to detect and track, and more prone to failures,
such as missed inspection and tracking loss.

Object aspect ratio. Table I and Fig. 2 also show the
object aspect ratio. The objects in our dataset have various
aspect ratios, where the maximum is 6.67, and the minimum
is 1.0. In one sequence, the same object has a signiﬁcant

aspect ratio change. For example, the object aspect ratio in
”video10” changes between 1.0 and 4.33. The aspect ratios of
most objects are between 1.0 and 3.0.

Object position. Fig. 1 describes the position distribution
of the objects’ relative center location in the form of scatter
plots. Most of the objects are concentrated in the center of the
image. The ranges of the object motion in all sets vary, and
the horizontal and vertical movements of objects are evenly
distributed. For the tracking dataset, the bounding boxes of the
object in one sequence are continuous. According to Fig. 1 (d),
in addition to the central area of the image, objects also move
frequently to the right and bottom-left of the image.

C. Dataset challenges

Through the analysis of the characteristics of the proposed
dataset in the last subsection, we ﬁnd that UAV detection and
tracking encounter many difﬁculties and challenges. The main
challenges are that the object is too small, the background
is complex or similar to the object, and the light changes
obviously. Object blur, fast motion, camera motion, and out
of view are also prone to occur. Fig. 4 and Fig. 5 respectively
show examples of the detection and tracking datasets that
reﬂect the aforementioned challenges.

IV. EXPERIMENTS

A. Detection on DUT Anti-UAV dataset

We select several state-of-the-art detection methods. We
use Faster-RCNN [11], Cascade-RCNN [41], and ATSS [42],
which are two-stage methods, and YOLOX [43] and SSD [13],

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

6

Fig. 5. Examples of tracking sequences and annotations of our dataset.

(a) IoU=0.5

(b) IoU=0.75

&

&

Fig. 6. P-R curves of all detectors.

which are one-stage methods. Two-stage models typically
have higher accuracy, while one-stage models perform better
in terms of the speed. Descriptions of these algorithms are
provided below.

Faster-RCNN [11]. This method makes several improve-
ments to Fast-RCNN [44] by resolving time-consuming issues
on region proposals brought by selective search. Instead of
selective search, the region proposal network (RPN) is pro-
posed. This network has two branches, namely, classiﬁcation
and regression. Classiﬁcation and regression are performed
twice, so the precision of the method is high.

Cascade-RCNN [41]. It consists of a series of detectors
with increasing Intersection over Union (IoU) thresholds. The
detectors are trained stage by stage, and the output of a
detector is the input of the next in which the IoU threshold
is higher (in other words, a detector with higher quality).
This method guarantees the amount of every detector, thereby
reducing the overﬁtting problem.

ATSS [42]. It claims that the essential difference between
anchor-based and anchor-free detectors is the way of deﬁning
positive and negative training samples. It proposes an algo-
rithm that can select positive and negative samples according

#100#177#555#906#1#150#320#450#196#197#208#307#82#280#572#5900.00.20.40.60.81.0recall0.00.20.40.60.81.0precisonATSS-ResNet18ATSS-ResNet50ATSS-VGG16Cascade-RCNN-ResNet18Cascade-RCNN-ResNet50Cascade-RCNN-VGG16Faster-RCNN-ResNet18Faster-RCNN-ResNet50Faster-RCNN-VGG16SSD-VGG16YOLOX-DarkNetYOLOX-ResNet18YOLOX-ResNet50YOLOX-VGG160.00.20.40.60.81.0recall0.00.20.40.60.81.0precisonATSS-ResNet18ATSS-ResNet50ATSS-VGG16Cascade-RCNN-ResNet18Cascade-RCNN-ResNet50Cascade-RCNN-VGG16Faster-RCNN-ResNet18Faster-RCNN-ResNet50Faster-RCNN-VGG16SSD-VGG16YOLOX-DarkNetYOLOX-ResNet18YOLOX-ResNet50YOLOX-VGG16IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

7

TABLE II
DETECTION RESULTS OF DIFFERENT COMBINATIONS OF MODELS AND
BACKBONES. THE TOP RESULTS OF MAP AND FPS ARE MARKS AS RED.

Faster-RCNN

Cascade-RCNN

ATSS

YOLOX

SSD

ResNet50
ResNet18
VGG16
ResNet50
ResNet18
VGG16
ResNet50
ResNet18
VGG16
ResNet50
ResNet18
VGG16
DarkNet
VGG16

mAP
0.653
0.605
0.633
0.683
0.652
0.667
0.642
0.61
0.641
0.427
0.400
0.551
0.552
0.632

FPS (tasks/s)
12.8
19.4
9.3
10.7
14.7
8.0
13.3
20.5
9.5
21.7
53.7
23.0
51.3
33.2

to the object’s statistical feature.

YOLO [43]. YOLO series is known for its extremely high
speed and relatively high accuracy. With the development of
object detection, it can integrate most advanced technologies,
so as to achieve rounds of iteration. After YOLOv5 reaches
a peak performance, YOLOX [43] starts to focus on anchor-
free detectors, advanced label assignment strategies, and end-
to-end (NMS-free) detectors, which are major advances in
these years. After upgrading, it shows remarkable performance
compared to YOLOv3 [30] on COCO (a detection dataset
named Common Objects in Context) [37].

SSD [13]. It is also a one-stage detector. It combines several
feature maps with different resolutions, thus improving the
model’s performance via multi-scale training. It has a good
effect on the detection of objects with different sizes. Only a
single network is involved, making the model easy to train.

We replace these detectors’ backbone network with sev-
eral classic backbone networks,
including ResNet18 [45],
ResNet50 [45], and VGG16 [46], and obtain 14 different ver-
sions of detection methods. The 14 detectors are all retrained
on the training subset of the DUT Anti-UAV detection dataset.
Moreover, we use mean average precision (mAP) and frames
per second (FPS) to evaluate the methods’ performance. The
results are shown in Table II. Cascade-RCNN with ResNet50
performs the best, and YOLOX with ResNet18 is the fastest.
We also visualize the performance of different detectors by
using P-R curves with different IoU thresholds, which are
shown in Fig. 6. In P-R curves, P means precision, and R
means recall. Typically, a negative correlation exists between
them, and a curve drawn with R as the abscissa and P as the
ordinate can effectively reﬂect the comprehensive performance
of a detector. Moreover, we illustrate several qualitative results
in Fig. 10. Faster-RCNN and Cascade-RCNN can get accurate
bounding boxes and corresponding conﬁdence scores, while
YOLO ofen detects the background as the target mistakenly.

B. Tracking on DUT Anti-UAV dataset

We select several existing state-of-the-art trackers and per-
form them on our tracking dataset. The tracking performance
is shown in the third column of Table III (the ”noDET”
column). We use three metric methods to evaluate the tracking

performance. First, the Success calculates the IoU between the
target ground truth and the predicted bounding boxes. It can
reﬂect the accuracy of the size and scale of the predicted target
bounding boxes. Second, the Precision measures the center
location error by computing the distance of pixels between
the ground truth and tracking results. However, it is easily
affected by target size and image resolution. To solve this
problem, the Norm Pre is introduced to rank trackers using
Area Under Curve (AUC) between 0 and 0.5.

We select seven tracking algorithms in total. Their descrip-

tions are provided below.

SiamFC [12]. This method is a classic generative tracking
algorithm based on a fully convolutional Siamese network.
It performs cross-correlation operation between the template
patch and the search region to locate the target. Besides, a
multi-scale strategy is used to decide the scale of the target.
SiamRPN++ [47]. It introduces the region proposal network
(RPN) into the Siamese network, and its backbone network
can be very deep. The framework has two branches, including
a classiﬁcation branch to choose the best anchor and a regres-
sion branch to predict offsets of the anchor. Compared with
SiamFC, SiamRPN++ is more robust and faster because of the
introduction of the RPN mechanism and the removal of the
multi-scale strategy.

ECO [48]. This method is a classic tracking algorithm based
on correlation ﬁltering. It introduces a factorized convolution
operator to reduce model parameters. A compact generative
model of the training sample space is proposed to reduce the
number of training samples while guaranteeing the diversity of
the sample set. Besides, an efﬁcient model update strategy is
also proposed to improve the tracker’s speed and robustness.
ATOM [49]. The model combines target classiﬁcation and
bounding box prediction. The former module is trained online
to guarantee a strong discriminative capability. The latter
module uses IoU loss and predicts the overlap between the
target and the predicted bounding box through ofﬂine training.
This combination endows the tracker with high discriminative
power and good regression capability.

DiMP [14]. Based on ATOM, this method introduces a
discriminative learning loss to guide the network to learn more
discriminative features. An efﬁcient optimizer is also designed
to accelerate the convergence of the network, which improves
the performance of the algorithm further.

TransT [50]. TransT is a Transformer-based method. Due
to its attention-based feature fusion network, this method is
able to extract abundant semantic feature maps, and achieves
state-of-the-art performance on most tracking benchmarks.

SPLT [51]. It is a long-term tracker mainly based on two
modules, namely, the perusal module and skimming module.
The perusal module contains an efﬁcient bounding box regres-
sor to generate a series of target proposals, and a target veriﬁer
is used to select the best one based on conﬁdence scores. The
skimming module is used to justify the state of the target
in the current frame and select an appropriate searching way
(global search or local search). These improve the speed of
the method, making it track in real-time.

LTMU [52]. It

is also a long-term tracker. The main
contribution of the method is to propose a meta-updater trained

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

8

Fig. 7. Framework of the proposed fusion strategy of tracking and detection.

ofﬂine, which is used to justify whether the tracker needs
to update in the current frame or not. It greatly improves
the robustness of the tracker. Moreover, a long-term tracking
framework is designed based on a SiamRPN-based re-detector,
an online veriﬁer, and an online local tracker with the proposed
meta-updater. This method shows its strong discriminative
capability and robustness on both long-term and short-term
tracking benchmarks.

We can ﬁnd that LTMU performs the best on our tracking
dataset, where the Success is 0.608 and Norm Pre is 0.783.
TransT, DiMP and ATOM also exhibit well performance, with
0.586, 0.578 and 0.574 in terms of Success, respectively. The
performance of SiamFC is the worst, in which the Success is
0.381 and Precision is 0.623.

C. Tracking with detection

To improve the tracking performance further, and make full
use of our dataset, including the detection and tracking sets,
we propose a clear and simple tracking algorithm combined
with detection. The fusion strategy is shown as Fig. 7 and
Algorithm 1. Given a tracker T and a detector D, we ﬁrst
initialize the tracker T based on the ground truth GT0 of the
ﬁrst frame. For each subsequent frame, we obtain the bounding
box bboxt and its conﬁdence score scoret from the tracker. If
scoret is less than τt, we regard it as an unreliable result, and
introduce the detection mechanism. Next, the detector obtains
bounding boxes bboxesd and their conﬁdence scores scoresd.
If the highest score scored is higher than τd and scoret, we
set the corresponding detected bounding box bboxd as the
current result; otherwise, bboxt is the ﬁnal result. In this paper,
hyper-parameters τt and τd are set to 0.9. To investigate the
effects of different parameter values for our fusion method,
we change values of hyper-parameters τt and τd, respectively.
Fig. 8 shows that our tracker is robust to these two parameters.
That is, huge changes in parameters τt and τd only cause slight
ﬂuctuations in the tracking results (less than 1%).

On the basis of the proposed fusion strategy, we try different
combinations of a series of trackers and detectors. Speciﬁcally,
we select the eight aforementioned trackers (SiamFC, ECO,
SPLT, ATOM, SiamRPN++, DiMP, TransT and LTMU) and
ﬁve detectors with different types of backbone networks (14
different versions in total). The detailed tracking results are
shown in Table III. The success and precision plots of each
tracker are shown as Fig. 9. After fusing detection, the tracking
performance of all trackers is improved signiﬁcantly. For in-
stance, compared with the baseline tracker SiamFC, the fused
method SiamFC+Faster-RCNN(VGG16) increases by 23.4%
in terms of the Success. The best-performing tracker LTMU

Fig. 8. Effects of different parameter values for our fusion method.

also improves its performance further after fusing the detector
Faster-RCNN(VGG16). The degree of tracking performance
improvement depends on the detection algorithm. For most
trackers, Faster-RCNN is a better fusion choice, especially
for its VGG16 version. On the contrary, ATSS hardly pro-
vides additional performance beneﬁts to trackers. Aside from
Faster-RCNN, Cascade-RCNN can also enhance the tracking
performance. Fig. 11 shows the qualitative comparison of the
original trackers and our fusion methods, where the model
Faster-RCNN-VGG16 is chosen as the fused detector. After
fusing detection, trackers can perform better in most scenarios.
Among them, the performance of LTMU-DET is best that can
handle most challenges.

R{0,N −1}

T , D, I{0,N −1}, GT0

Algorithm 1 Fusion strategy of trackers and detectors
Input:
Output:
1: T .init(I0,GT0);
2: R0 = GT0;
3: for each t ∈ [1, N − 1] do
4:
5:
6:
7:

bboxt, scoret = T .track(It);
if scoret < τt then

bboxesd, scoresd = D.detect(It);
if length(bboxesd) > 0 then
scored = Max(scoresd);
Collect the corresponding bboxd from bboxesd;
if scored > τd and scored > scoret then

8:
9:
10:
11:
12:
13:

Rt = bboxd;

else

Rt = bboxt;

end if

end if

14:
15:
end if
16:
17: end for

V. CONCLUSION

In this paper, we propose the DUT Anti-UAV dataset for
UAV detection and tracking. It contains two sets, namely,
detection and tracking. The former has 10,109 objects from
10,000 images, which are split into three subsets (training,
testing, and validation). The latter contains 20 sequences
whose average length is 1240. All images and frames are
annotated manually and precisely. We set 14 different ver-
sions of the detectors from the combination of 5 types of

TrackerDetector#T#T𝑠𝑐𝑜𝑟𝑒𝑡#T𝑠𝑐𝑜𝑟𝑒𝑑𝑠𝑐𝑜𝑟𝑒𝑡<𝜏𝑡𝑠𝑐𝑜𝑟𝑒𝑑>𝜏𝑑and𝑠𝑐𝑜𝑟𝑒𝑑>𝑠𝑐𝑜𝑟𝑒𝑡𝑅𝑇=𝑏𝑏𝑜𝑥𝑡𝑅𝑇=𝑏𝑏𝑜𝑥𝑑noyesyesno0.6670.6680.6660.6660.6640.6620.6630.6640.6620.6660.6640.6590.650.6550.660.6650.670.50.60.70.80.90.95SuccessParameter valueτ_dτ_t𝜏d𝜏𝑡IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

9

(a) SiamFC

(b) ECO

(c) SPLT

(d) ATOM

(e) SiamRPN++

(f) TransT

Fig. 9. Success and precision plots of trackers on the DUT Anti-UAV dataset.

(g) DiMP

(h) LTMU

0.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.80.9SuccessrateSuccessplotsofOPEonAnti-UAV[0.617]SiamFC-Cascade-RCNN-ResNet50[0.616]SiamFC-Faster-RCNN-ResNet50[0.615]SiamFC-Faster-RCNN-VGG16[0.611]SiamFC-Cascade-RCNN-VGG16[0.607]SiamFC-Cascade-RCNN-ResNet18[0.605]SiamFC-Faster-RCNN-ResNet18[0.551]SiamFC-SSD-VGG16[0.383]SiamFC-YOLOX-VGG16[0.381]SiamFC-ATSS-ResNet50[0.381]SiamFC-YOLOX-DarkNet[0.381]SiamFC-YOLOX-ResNet18[0.381]SiamFC-ATSS-VGG16[0.381]SiamFC-YOLOX-ResNet50[0.381]SiamFC-noDET[0.381]SiamFC-ATSS-ResNet1805101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.80.9PrecisionPrecisionplotsofOPEonAnti-UAV[0.943]SiamFC-Faster-RCNN-VGG16[0.937]SiamFC-Faster-RCNN-ResNet50[0.933]SiamFC-Cascade-RCNN-ResNet50[0.930]SiamFC-Cascade-RCNN-VGG16[0.923]SiamFC-Faster-RCNN-ResNet18[0.920]SiamFC-Cascade-RCNN-ResNet18[0.832]SiamFC-SSD-VGG16[0.625]SiamFC-YOLOX-VGG16[0.623]SiamFC-ATSS-ResNet50[0.623]SiamFC-YOLOX-DarkNet[0.623]SiamFC-YOLOX-ResNet18[0.623]SiamFC-ATSS-VGG16[0.623]SiamFC-YOLOX-ResNet50[0.623]SiamFC-noDET[0.623]SiamFC-ATSS-ResNet180.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.80.9SuccessrateSuccessplotsofOPEonAnti-UAV[0.620]ECO-Faster-RCNN-VGG16[0.620]ECO-Cascade-RCNN-ResNet50[0.619]ECO-Faster-RCNN-ResNet50[0.618]ECO-Cascade-RCNN-VGG16[0.614]ECO-Cascade-RCNN-ResNet18[0.610]ECO-Faster-RCNN-ResNet18[0.578]ECO-SSD-VGG16[0.436]ECO-YOLOX-VGG16[0.417]ECO-ATSS-ResNet50[0.414]ECO-ATSS-ResNet18[0.411]ECO-ATSS-VGG16[0.410]ECO-YOLOX-ResNet50[0.401]ECO-YOLOX-DarkNet[0.400]ECO-YOLOX-ResNet18[0.395]ECO-noDET05101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.80.9PrecisionPrecisionplotsofOPEonAnti-UAV[0.954]ECO-Faster-RCNN-VGG16[0.946]ECO-Faster-RCNN-ResNet50[0.945]ECO-Cascade-RCNN-VGG16[0.942]ECO-Cascade-RCNN-ResNet50[0.938]ECO-Cascade-RCNN-ResNet18[0.937]ECO-Faster-RCNN-ResNet18[0.884]ECO-SSD-VGG16[0.749]ECO-YOLOX-VGG16[0.726]ECO-ATSS-ResNet50[0.718]ECO-YOLOX-ResNet50[0.717]ECO-ATSS-VGG16[0.717]ECO-ATSS-ResNet18[0.706]ECO-YOLOX-ResNet18[0.705]ECO-YOLOX-DarkNet[0.697]ECO-noDET0.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.8SuccessrateSuccessplotsofOPEonAnti-UAV[0.607]SPLT-YOLOX-DarkNet[0.605]SPLT-YOLOX-ResNet50[0.603]SPLT-YOLOX-ResNet18[0.597]SPLT-YOLOX-VGG16[0.553]SPLT-Faster-RCNN-VGG16[0.550]SPLT-Faster-RCNN-ResNet50[0.549]SPLT-Cascade-RCNN-ResNet50[0.546]SPLT-Cascade-RCNN-VGG16[0.543]SPLT-Faster-RCNN-ResNet18[0.542]SPLT-Cascade-RCNN-ResNet18[0.496]SPLT-SSD-VGG16[0.405]SPLT-ATSS-ResNet50[0.405]SPLT-noDET[0.405]SPLT-ATSS-VGG16[0.405]SPLT-ATSS-ResNet1805101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.8PrecisionPrecisionplotsofOPEonAnti-UAV[0.875]SPLT-Faster-RCNN-VGG16[0.871]SPLT-Faster-RCNN-ResNet50[0.866]SPLT-Cascade-RCNN-ResNet50[0.862]SPLT-Cascade-RCNN-VGG16[0.861]SPLT-YOLOX-ResNet50[0.860]SPLT-Faster-RCNN-ResNet18[0.857]SPLT-YOLOX-DarkNet[0.855]SPLT-YOLOX-ResNet18[0.855]SPLT-Cascade-RCNN-ResNet18[0.846]SPLT-YOLOX-VGG16[0.778]SPLT-SSD-VGG16[0.651]SPLT-ATSS-ResNet50[0.651]SPLT-ATSS-ResNet18[0.651]SPLT-noDET[0.651]SPLT-ATSS-VGG160.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.80.9SuccessrateSuccessplotsofOPEonAnti-UAV[0.635]ATOM-Faster-RCNN-ResNet18[0.633]ATOM-Faster-RCNN-ResNet50[0.632]ATOM-Faster-RCNN-VGG16[0.626]ATOM-Cascade-RCNN-VGG16[0.621]ATOM-Cascade-RCNN-ResNet50[0.611]ATOM-Cascade-RCNN-ResNet18[0.601]ATOM-SSD-VGG16[0.574]ATOM-YOLOX-ResNet18[0.565]ATOM-ATSS-VGG16[0.547]ATOM-YOLOX-DarkNet[0.543]ATOM-ATSS-ResNet50[0.537]ATOM-YOLOX-VGG16[0.536]ATOM-YOLOX-ResNet50[0.532]ATOM-ATSS-ResNet18[0.509]ATOM-noDET05101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.80.9PrecisionPrecisionplotsofOPEonAnti-UAV[0.936]ATOM-Faster-RCNN-ResNet18[0.932]ATOM-Faster-RCNN-VGG16[0.931]ATOM-Faster-RCNN-ResNet50[0.918]ATOM-Cascade-RCNN-VGG16[0.917]ATOM-Cascade-RCNN-ResNet50[0.895]ATOM-Cascade-RCNN-ResNet18[0.870]ATOM-SSD-VGG16[0.836]ATOM-YOLOX-ResNet18[0.832]ATOM-ATSS-VGG16[0.809]ATOM-ATSS-ResNet50[0.808]ATOM-YOLOX-DarkNet[0.796]ATOM-YOLOX-ResNet50[0.794]ATOM-YOLOX-VGG16[0.774]ATOM-ATSS-ResNet18[0.741]ATOM-noDET0.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.8SuccessrateSuccessplotsofOPEonAnti-UAV[0.612]SiamRPN++-Faster-RCNN-VGG16[0.610]SiamRPN++-Faster-RCNN-ResNet50[0.610]SiamRPN++-Cascade-RCNN-VGG16[0.610]SiamRPN++-Cascade-RCNN-ResNet50[0.606]SiamRPN++-Faster-RCNN-ResNet18[0.606]SiamRPN++-Cascade-RCNN-ResNet18[0.591]SiamRPN++-SSD-VGG16[0.546]SiamRPN++-YOLOX-VGG16[0.545]SiamRPN++-ATSS-ResNet50[0.545]SiamRPN++-ATSS-ResNet18[0.545]SiamRPN++-YOLOX-ResNet50[0.545]SiamRPN++-YOLOX-DarkNet[0.545]SiamRPN++-YOLOX-ResNet18[0.545]SiamRPN++-noDET[0.545]SiamRPN++-ATSS-VGG1605101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.8PrecisionPrecisionplotsofOPEonAnti-UAV[0.881]SiamRPN++-Faster-RCNN-VGG16[0.877]SiamRPN++-Faster-RCNN-ResNet50[0.876]SiamRPN++-Cascade-RCNN-VGG16[0.876]SiamRPN++-Cascade-RCNN-ResNet50[0.873]SiamRPN++-Faster-RCNN-ResNet18[0.870]SiamRPN++-Cascade-RCNN-ResNet18[0.843]SiamRPN++-SSD-VGG16[0.781]SiamRPN++-YOLOX-VGG16[0.780]SiamRPN++-ATSS-ResNet50[0.780]SiamRPN++-ATSS-ResNet18[0.780]SiamRPN++-YOLOX-ResNet50[0.780]SiamRPN++-YOLOX-DarkNet[0.780]SiamRPN++-YOLOX-ResNet18[0.780]SiamRPN++-noDET[0.780]SiamRPN++-ATSS-VGG160.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.8SuccessrateSuccessplotsofOPEonAnti-UAV[0.624]TransT-Cascade-RCNN-ResNet50[0.623]TransT-Faster-RCNN-VGG16[0.623]TransT-Faster-RCNN-ResNet50[0.623]TransT-Cascade-RCNN-VGG16[0.623]TransT-Faster-RCNN-ResNet18[0.623]TransT-Cascade-RCNN-ResNet18[0.586]TransT-YOLOX-VGG16[0.586]TransT-ATSS-ResNet50[0.586]TransT-ATSS-ResNet18[0.586]TransT-SSD-VGG16[0.586]TransT-YOLOX-ResNet50[0.586]TransT-YOLOX-ResNet18[0.586]TransT-ATSS-VGG16[0.586]TransT-YOLOX-DarkNet[0.586]TransT-noDET05101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.8PrecisionPrecisionplotsofOPEonAnti-UAV[0.888]TransT-Faster-RCNN-VGG16[0.888]TransT-Faster-RCNN-ResNet50[0.888]TransT-Cascade-RCNN-ResNet50[0.887]TransT-Cascade-RCNN-VGG16[0.886]TransT-Faster-RCNN-ResNet18[0.885]TransT-Cascade-RCNN-ResNet18[0.832]TransT-YOLOX-VGG16[0.832]TransT-ATSS-ResNet50[0.832]TransT-ATSS-ResNet18[0.832]TransT-SSD-VGG16[0.832]TransT-YOLOX-ResNet50[0.832]TransT-YOLOX-ResNet18[0.832]TransT-ATSS-VGG16[0.832]TransT-YOLOX-DarkNet[0.832]TransT-noDET0.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.80.9SuccessrateSuccessplotsofOPEonAnti-UAV[0.657]DiMP-Faster-RCNN-ResNet50[0.654]DiMP-Faster-RCNN-VGG16[0.647]DiMP-Faster-RCNN-ResNet18[0.645]DiMP-Cascade-RCNN-ResNet18[0.637]DiMP-Cascade-RCNN-ResNet50[0.632]DiMP-Cascade-RCNN-VGG16[0.627]DiMP-SSD-VGG16[0.609]DiMP-ATSS-ResNet50[0.608]DiMP-noDET[0.600]DiMP-ATSS-VGG16[0.598]DiMP-YOLOX-ResNet18[0.595]DiMP-YOLOX-VGG16[0.589]DiMP-ATSS-ResNet18[0.589]DiMP-YOLOX-ResNet50[0.583]DiMP-YOLOX-DarkNet05101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.80.9PrecisionPrecisionplotsofOPEonAnti-UAV[0.949]DiMP-Faster-RCNN-ResNet50[0.945]DiMP-Faster-RCNN-VGG16[0.936]DiMP-Faster-RCNN-ResNet18[0.931]DiMP-Cascade-RCNN-ResNet18[0.915]DiMP-Cascade-RCNN-VGG16[0.912]DiMP-Cascade-RCNN-ResNet50[0.898]DiMP-SSD-VGG16[0.873]DiMP-ATSS-ResNet50[0.861]DiMP-noDET[0.852]DiMP-ATSS-VGG16[0.851]DiMP-YOLOX-ResNet18[0.845]DiMP-YOLOX-VGG16[0.838]DiMP-ATSS-ResNet18[0.836]DiMP-YOLOX-ResNet50[0.829]DiMP-YOLOX-DarkNet0.00.10.20.30.40.50.60.70.80.91.0Overlapthreshold0.00.10.20.30.40.50.60.70.80.9SuccessrateSuccessplotsofOPEonAnti-UAV[0.664]LTMU-Faster-RCNN-VGG16[0.659]LTMU-Cascade-RCNN-ResNet50[0.659]LTMU-Faster-RCNN-ResNet50[0.658]LTMU-Cascade-RCNN-VGG16[0.657]LTMU-Cascade-RCNN-ResNet18[0.653]LTMU-Faster-RCNN-ResNet18[0.623]LTMU-SSD-VGG16[0.612]LTMU-YOLOX-ResNet18[0.608]LTMU-noDET[0.606]LTMU-YOLOX-DarkNet[0.606]LTMU-YOLOX-VGG16[0.605]LTMU-ATSS-ResNet18[0.605]LTMU-ATSS-ResNet50[0.600]LTMU-YOLOX-ResNet50[0.600]LTMU-ATSS-VGG1605101520253035404550Locationerrorthreshold0.00.10.20.30.40.50.60.70.80.9PrecisionPrecisionplotsofOPEonAnti-UAV[0.961]LTMU-Faster-RCNN-VGG16[0.954]LTMU-Cascade-RCNN-ResNet50[0.954]LTMU-Faster-RCNN-ResNet50[0.951]LTMU-Cascade-RCNN-VGG16[0.948]LTMU-Cascade-RCNN-ResNet18[0.946]LTMU-Faster-RCNN-ResNet18[0.887]LTMU-SSD-VGG16[0.874]LTMU-YOLOX-ResNet18[0.858]LTMU-YOLOX-VGG16[0.858]LTMU-noDET[0.856]LTMU-ATSS-ResNet18[0.855]LTMU-YOLOX-DarkNet[0.855]LTMU-ATSS-ResNet50[0.847]LTMU-ATSS-VGG16[0.845]LTMU-YOLOX-ResNet50IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

10

Fig. 10. Qualitative comparison of detection results. The ﬁrst row to the last row indicate the detection results of Faster-RCNN-ResNet50, Cascade-RCNN-
ResNet50, ATSS-ResNet50, SSD-VGG16 and YOLO-DarkNet in turn, including the target bounding box and the corresponding conﬁdence score. Better
viewed with zoom-in.

Fig. 11. Qualitative comparison of tracking results. ”noDET” means pure tracking results without fusing detection, and we choose the model Faster-RCNN-
VGG16 for ”DET” results. Better viewed in color with zoom-in.

UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 1.00UAV | 0.83UAV | 0.80UAV | 0.77UAV | 0.68UAV | 0.81UAV | 0.70UAV | 0.68UAV | 0.81UAV | 0.96UAV | 1.00UAV | 1.00UAV | 1.00UAV | 0.98UAV | 1.00UAV | 1.00UAV | 0.86UAV | 0.41UAV | 0.99UAV | 0.90UAV | 0.94UAV | 0.61UAV | 0.76UAV | 0.21UAV | 0.81UAV | 0.84#169#184#200#368#72#218#284#337#343#351#413#706#744#842#896#1042GroundTruthSiamFC-noDETSiamFC-DETSPLT-noDETSPLT-DETATOM-noDETATOM-DETSiamRPN++-noDETSiamRPN++-DETLTMU-noDETLTMU-DETIEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

11

TABLE III
TRACKING RESULTS OF DIFFERENT COMBINATIONS OF TRACKERS AND DETECTORS. THE TOP TRACKING RESULTS OF EACH TRACKER ARE MARKS AS
BLUE, AND THE BEST RESULTS OF ALL COMBINATIONS ARE MARKED AS RED.

SiamFC

ECO

SPLT

ATOM

SiamRPN++

DiMP

TransT

LTMU

Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision
Success
Norm Pre
Precision

noDET

0.381
0.526
0.623
0.404
0.643
0.717
0.405
0.585
0.651
0.574
0.758
0.830
0.545
0.709
0.780
0.578
0.756
0.831
0.586
0.765
0.832
0.608
0.783
0.858

ResNet18
0.381
0.526
0.623
0.414
0.655
0.717
0.405
0.585
0.651
0.532
0.703
0.774
0.545
0.709
0.780
0.589
0.760
0.838
0.586
0.765
0.832
0.605
0.782
0.856

ATSS
ResNet50
0.381
0.526
0.623
0.417
0.663
0.726
0.405
0.585
0.651
0.543
0.742
0.809
0.545
0.709
0.780
0.609
0.796
0.873
0.586
0.765
0.832
0.605
0.780
0.855

VGG16
0.381
0.526
0.623
0.411
0.655
0.717
0.405
0.585
0.651
0.565
0.767
0.832
0.545
0.709
0.780
0.600
0.773
0.852
0.586
0.765
0.832
0.600
0.768
0.847

ResNet18
0.607
0.792
0.920
0.614
0.809
0.938
0.542
0.766
0.855
0.611
0.791
0.895
0.606
0.788
0.870
0.645
0.840
0.931
0.623
0.807
0.885
0.657
0.855
0.948

Cascade-RCNN
ResNet50
0.617
0.800
0.933
0.620
0.809
0.942
0.549
0.775
0.866
0.621
0.814
0.917
0.610
0.793
0.876
0.637
0.818
0.912
0.624
0.808
0.888
0.659
0.856
0.954

VGG16
0.611
0.802
0.930
0.618
0.817
0.945
0.546
0.772
0.862
0.626
0.814
0.918
0.610
0.793
0.876
0.632
0.825
0.915
0.623
0.808
0.888
0.658
0.856
0.951

ResNet18
0.605
0.793
0.923
0.610
0.806
0.937
0.543
0.770
0.860
0.635
0.828
0.936
0.606
0.789
0.873
0.647
0.845
0.936
0.623
0.807
0.886
0.653
0.851
0.946

Faster-RCNN
ResNet50
0.616
0.804
0.937
0.619
0.813
0.946
0.550
0.779
0.871
0.633
0.820
0.931
0.610
0.794
0.877
0.657
0.856
0.949
0.623
0.808
0.888
0.659
0.858
0.954

VGG16
0.615
0.811
0.943
0.620
0.821
0.954
0.553
0.783
0.875
0.632
0.823
0.932
0.612
0.797
0.881
0.654
0.850
0.945
0.623
0.808
0.888
0.664
0.865
0.961

SSD
VGG16
0.551
0.708
0.832
0.578
0.786
0.884
0.496
0.700
0.778
0.601
0.778
0.870
0.591
0.766
0.843
0.627
0.809
0.898
0.586
0.765
0.832
0.623
0.803
0.887

ResNet18
0.381
0.526
0.623
0.400
0.643
0.706
0.603
0.779
0.855
0.574
0.763
0.836
0.545
0.709
0.780
0.598
0.772
0.851
0.586
0.765
0.832
0.612
0.799
0.874

YOLOX

ResNet50
0.381
0.526
0.623
0.410
0.656
0.718
0.605
0.780
0.861
0.536
0.730
0.796
0.545
0.709
0.780
0.589
0.761
0.836
0.586
0.765
0.832
0.600
0.772
0.845

VGG16
0.383
0.528
0.625
0.436
0.687
0.749
0.597
0.769
0.846
0.537
0.722
0.794
0.546
0.710
0.781
0.595
0.768
0.845
0.586
0.765
0.832
0.606
0.783
0.858

DarkNet
0.381
0.526
0.623
0.401
0.644
0.705
0.607
0.783
0.857
0.547
0.744
0.808
0.545
0.709
0.780
0.583
0.753
0.829
0.586
0.765
0.832
0.606
0.781
0.855

detection algorithms and 3 types of backbone networks. These
methods are retrained using our detection-training dataset and
evaluated on our detection-testing dataset. Besides, we present
the tracking results of the 8 trackers on our tracking dataset.
To improve the tracking performance further and make full
use of our detection and tracking datasets, we propose a
simple and clear fusion strategy with trackers and detectors
and evaluate the tracking results of the combinations of 8
trackers and 14 detectors. Extensive experiments show that
our fusion strategy can improve the tracking performance of
all trackers signiﬁcantly.

REFERENCES

[1] J. P. ˇSkrinjar, P. ˇSkorput, and M. Furdi´c, “Application of unmanned
aerial vehicles in logistic processes,” in International Conference “New
Technologies, Development and Applications”, 2018, pp. 359–366.
[2] Y. Xu, G. Yu, Y. Wang, X. Wu, and Y. Ma, “Car detection from low-
altitude UAV imagery with the faster R-CNN,” Journal of Advanced
Transportation, vol. 2017, pp. 1–10, 2017.

[3] H. Cheng, L. Lin, Z. Zheng, Y. Guan, and Z. Liu, “An autonomous
vision-based target tracking system for rotorcraft unmanned aerial vehi-
cles,” in IEEE/RSJ International Conference on Intelligent Robots and
Systems, 2017, pp. 1732–1738.

[4] M. Lort, A. Aguasca, C. Lopez-Martinez, and T. M. Mar´ın, “Initial
evaluation of sar capabilities in UAV multicopter platforms,” Journal
of Selected Topics in Applied Earth Observations and Remote Sensing,
vol. 11, no. 1, pp. 127–140, 2017.

[5] F. Hoffmann, M. Ritchie, F. Fioranelli, A. Charlish, and H. Grifﬁths,
“Micro-doppler based detection and tracking of UAVs with multistatic
radar,” in IEEE Radar Conference, 2016, pp. 1–6.

[6] A. H. Abunada, A. Y. Osman, A. Khandakar, M. E. H. Chowdhury,
T. Khattab, and F. Touati, “Design and implementation of a rf based
anti-drone system,” in IEEE International Conference on Informatics,
IoT, and Enabling Technologies, 2020, pp. 35–42.

[7] X. Chang, C. Yang, J. Wu, X. Shi, and Z. Shi, “A surveillance system
for drone localization and tracking using acoustic arrays,” in IEEE 10th
Sensor Array and Multichannel Signal Processing Workshop, 2018, pp.
573–577.

[8] G. Gao, Y. Yu, M. Yang, H. Chang, P. Huang, and D. Yue, “Cross-
resolution face recognition with pose variations via multilayer locality-
constrained structural orthogonal procrustes regression,” Information
Sciences, vol. 506, pp. 19–36, 2020.

[9] G. Gao, Y. Yu, J. Xie, J. Yang, M. Yang, and J. Zhang, “Constructing
multilayer locality-constrained matrix regression framework for noise
robust face super-resolution,” Pattern Recognition, vol. 110, p. 107539,
2021.

[10] G. Gao, Y. Yu, J. Yang, G.-J. Qi, and M. Yang, “Hierarchical deep
cnn feature set-based representation learning for robust cross-resolution
face recognition,” IEEE Transactions on Circuits and Systems for Video
Technology, 2020.

[11] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” Advances in
Neural Information Processing Systems, vol. 28, 2015.

[12] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,
“Fully-convolutional siamese networks for object tracking,” in European
Conference on Computer Vision, 2016, pp. 850–865.

[13] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg, “Ssd: Single shot multibox detector,” in European Conference on
Computer Vision, 2016, pp. 21–37.

[14] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, “Learning discrimi-
native model prediction for tracking,” in IEEE International Conference
on Computer Vision, 2019, pp. 6182–6191.

[15] Y. Hu, X. Wu, G. Zheng, and X. Liu, “Object detection of UAV for
anti-UAV based on improved yolo v3,” in Chinese Control Conference,
2019, pp. 8386–8390.

[16] C. Wang, T. Wang, E. Wang, E. Sun, and Z. Luo, “Flying small
target detection for anti-UAV based on a gaussian mixture model in
a compressive sensing domain,” Sensors, vol. 19, no. 9, p. 2168, 2019.
[17] N. Jiang, K. Wang, X. Peng, X. Yu, Q. Wang, J. Xing, G. Li, J. Zhao,
G. Guo, and Z. Han, “Anti-UAV: A large multi-modal benchmark for
UAV tracking,” arXiv preprint arXiv:2101.08466, 2021.

[18] A. Rodriguez-Ramos, J. Rodriguez-Vazquez, C. Sampedro, and P. Cam-
poy, “Adaptive inattentional framework for video object detection with
reward-conditional training,” IEEE Access, vol. 8, pp. 124 451–124 466,
2020.

[19] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for
UAV tracking,” in European Conference on Computer Vision, 2016, pp.
445–461.

[20] I. Kalra, M. Singh, S. Nagpal, R. Singh, M. Vatsa, and P. Sujit,
“Dronesurf: Benchmark dataset for drone-based face recognition,” in
IEEE International Conference on Automatic Face & Gesture Recogni-
tion, 2019, pp. 1–7.

[21] M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, “Drone-based object counting by
spatially regularized regional proposal network,” in IEEE International
Conference on Computer Vision, 2017, pp. 4145–4153.

[22] H. Yu, G. Li, W. Zhang, Q. Huang, D. Du, Q. Tian, and N. Sebe,
“The unmanned aerial vehicle benchmark: Object detection, tracking
and baseline,” International Journal of Computer Vision, vol. 128, no. 5,
pp. 1141–1159, 2020.

[23] S. Li and D.-Y. Yeung, “Visual object tracking for unmanned aerial
vehicles: A benchmark and new motion models,” in AAAI Conference
on Artiﬁcial Intelligence, 2017.

[24] D. Xing, N. Evangeliou, A. Tsoukalas, and A. Tzes, “Siamese trans-
former pyramid networks for real-time UAV tracking,” arXiv preprint
arXiv:2110.08822, 2021.

[25] H. Yu, L. Qin, Q. Huang, and H. Yao, “Online multiple object tracking

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS

12

[48] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO:
Efﬁcient convolution operators for tracking,” in IEEE Conference on
Computer Vision and Pattern Recognition, 2017, pp. 6638–6646.
[49] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg, “ATOM: Accurate
tracking by overlap maximization,” in IEEE Conference on Computer
Vision and Pattern Recognition, 2019, pp. 4660–4669.

[50] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer
tracking,” in IEEE Conference on Computer Vision and Pattern Recog-
nition, 2021, pp. 8126–8135.

[51] B. Yan, H. Zhao, D. Wang, H. Lu, and X. Yang, “‘Skimming-perusal’
tracking: A framework for real-time and robust long-term tracking,” in
IEEE International Conference on Computer Vision, 2019, pp. 2385–
2393.

[52] K. Dai, Y. Zhang, D. Wang, J. Li, H. Lu, and X. Yang, “High-
performance long-term tracking with meta-updater,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2020, pp. 6298–6307.

via exchanging object context,” Neurocomputing, vol. 292, pp. 28–37,
2018.

[26] X. Shi, C. Yang, W. Xie, C. Liang, Z. Shi, and J. Chen, “Anti-drone
system with multiple surveillance technologies: Architecture, implemen-
tation, and challenges,” IEEE Communications Magazine, vol. 56, no. 4,
pp. 68–74, 2018.

[27] B.-H. Sheu, C.-C. Chiu, W.-T. Lu, C.-I. Huang, and W.-P. Chen,
“Development of UAV tracing and coordinate detection method using
a dual-axis rotary platform for an anti-UAV system,” Applied Sciences,
vol. 9, no. 13, p. 2583, 2019.

[28] A. Coluccia, A. Fascista, A. Schumann, L. Sommer, M. Ghenescu, T. Pi-
atrik, G. De Cubber, M. Nalamati, A. Kapoor, M. Saqib et al., “Drone-
vs-bird detection challenge at IEEE AVSS2019,” in IEEE International
Conference on Advanced Video and Signal Based Surveillance, 2019,
pp. 1–7.

[29] B. K. Isaac-Medina, M. Poyser, D. Organisciak, C. G. Willcocks, T. P.
Breckon, and H. P. Shum, “Unmanned aerial vehicle visual detection
and tracking using deep neural networks: A performance benchmark,”
in IEEE International Conference on Computer Vision, 2021, pp. 1223–
1232.

[30] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”

arXiv preprint arXiv:1804.02767, 2018.

[31] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S. Zagoruyko, “End-to-end object detection with transformers,” in
European Conference on Computer Vision, 2020, pp. 213–229.
[32] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online
and realtime tracking,” in IEEE International Conference on Image
Processing, 2016, pp. 3464–3468.

[33] N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime track-
ing with a deep association metric,” in IEEE International Conference
on Image Processing, 2017, pp. 3645–3649.

[34] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, “Tracking without bells
and whistles,” in IEEE International Conference on Computer Vision,
2019, pp. 941–951.

[35] J. Zhao, G. Wang, J. Li, L. Jin, N. Fan, M. Wang, X. Wang, T. Yong,
Y. Deng, Y. Guo et al., “The 2nd Anti-UAV workshop & challenge:
Methods and results,” arXiv preprint arXiv:2108.09909, 2021.

[36] B. Huang, J. Chen, T. Xu, Y. Wang, S. Jiang, Y. Wang, L. Wang,
and J. Li, “Siamsta: Spatio-temporal attention based siamese tracker for
tracking UAVs,” in IEEE International Conference on Computer Vision,
2021, pp. 1204–1212.

[37] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft CoCo: Common objects in
context,” in European Conference on Computer Vision, 2014, pp. 740–
755.

[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International Journal of Computer
Vision, vol. 115, no. 3, pp. 211–252, 2015.

[39] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu, C. Liao,
and H. Ling, “LaSOT: A high-quality benchmark for large-scale single
object tracking,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 5374–5383.

[40] Y. Wu, J. Lim, and M.-H. Yang, “Object tracking benchmark,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 37,
no. 9, pp. 1834–1848, 2015.

[41] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into high quality
object detection,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 6154–6162.

[42] S. Zhang, C. Chi, Y. Yao, Z. Lei, and S. Z. Li, “Bridging the gap
between anchor-based and anchor-free detection via adaptive training
sample selection,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2020, pp. 9759–9768.

[43] Z. Ge, S. Liu, F. Wang, Z. Li, and J. Sun, “Yolox: Exceeding yolo series

in 2021,” arXiv preprint arXiv:2107.08430, 2021.

[44] R. Girshick, “Fast R-CNN,” in IEEE International Conference on

Computer Vision, 2015, pp. 1440–1448.

[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 770–778.

[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[47] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++:
Evolution of siamese visual tracking with very deep networks,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4282–4291.

