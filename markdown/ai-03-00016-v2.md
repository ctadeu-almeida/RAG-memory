Article
Distinguishing Malicious Drones Using Vision Transformer

Sonain Jamil 1,*

, Muhammad Sohail Abbas 2 and Arunabha M. Roy 3,*

1 Department of Electronics Engineering, Sejong University, Seoul 05006, Korea
2

School of Electrical Engineering and Computer Science (SEECS), National University of Sciences and
Technology (NUST), Islamabad 44000, Pakistan; mabbas.mscs20seecs@seecs.edu.pk
3 Aerospace Engineering Department, University of Michigan, Ann Arbor, MI 48109, USA
* Correspondence: sonainjamil@sju.ac.kr (S.J.); arunabhr@umich.edu or arunabhr.umich@gmail.com (A.M.R.)

Abstract: Drones are commonly used in numerous applications, such as surveillance, navigation,
spraying pesticides in autonomous agricultural systems, various military services, etc., due to their
variable sizes and workloads. However, malicious drones that carry harmful objects are often
adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection
of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based
framework to distinguish between drones and malicious drones. In the proposed ViT based model,
drone images are split into Ô¨Åxed-size patches; then, linearly embeddings and position embeddings
are applied, and the resulting sequence of vectors is Ô¨Ånally fed to a standard ViT encoder. During
classiÔ¨Åcation, an additional learnable classiÔ¨Åcation token associated to the sequence is used. The
proposed framework is compared with several handcrafted and deep convolutional neural networks
(D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming
various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is
illustrated by comparing it with the existing state-of-the-art drone-detection methods.

Keywords: vision transformer; deep convolutional neural networks; deep learning; malicious drones;
classiÔ¨Åcation; drones

Citation: Jamil, S.; Abbas, M.S.; Roy,

A.M. Distinguishing Malicious

Drones Using Vision Transformer. AI

1. Introduction

2022, 3, 260‚Äì273. https://doi.org/

10.3390/ai3020016

Academic Editor: Rafa≈Ç Dre Àôzewski

Received: 6 March 2022

Accepted: 29 March 2022

Published: 31 March 2022

Publisher‚Äôs Note: MDPI stays neutral

with regard to jurisdictional claims in

published maps and institutional afÔ¨Ål-

iations.

Copyright: ¬© 2022 by the authors.

Licensee MDPI, Basel, Switzerland.

This article is an open access article

distributed under

the terms and

conditions of the Creative Commons

Attribution (CC BY) license (https://

creativecommons.org/licenses/by/

4.0/).

With recent development in remote sensing technology, drones play an essential role
in developing smart cities and innovative industries due to their numerous applications,
including automated irrigation, spraying pesticides and fertilizers in agriculture [1], water
management [2], food services [3], UAS-based image velocimetry [4], Ô¨Çying base stations [5],
etc. Drones of variable sizes and different shapes have been deployed in the military for
navigation and surveillance purposes [6].

In spite of numerous useful applications, drones are often used for spying and carrying
dangerous loads. Such drones are termed malicious drones, which enter in the restricted
non-Ô¨Çy zones avoiding radar detection due to their low-altitude Ô¨Çight path. The schematic
in Figure 1a,b shows the normal use cases of drones, and Figure 1c depicts the intrusion of
malicious drones in the restricted zones.

Hence, it is critical to develop an autonomous system that can efÔ¨Åciently detect the
intrusion of malicious drones to avoid any potential damage. In that regard, machine
learning (ML) and computer vision (CV) can allow us to develop automated systems
that can detect malicious drones. The existing techniques in the literature usually rely on
audios, images, videos, and radio frequency signals to detect drones. In [7], the authors
proposed a DL-based hybrid audio and integrated visual framework for detecting malicious
drones, which achieved an accuracy of 98.5% for the combined audio and visual dataset.
However, the main drawback of the model was that it was limited to drone detection, and
the model was unable to differentiate between drones with loads and without loads. Along
similar lines, in [8], the authors proposed a mel frequency cepstral coefÔ¨Åcient (MFCC)

AI 2022, 3, 260‚Äì273. https://doi.org/10.3390/ai3020016

https://www.mdpi.com/journal/ai

(cid:1)(cid:2)(cid:3)(cid:1)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:1)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)AI 2022, 3

261

with a SVM-based model for detecting malicious drones; however, the performance model
deteriorates when detecting amateur drones in adverse weather conditions and noisy
environments. Moreover, in [9], the authors proposed a handcrafted feature extraction-
based technique to detect drones using audios and images. The method achieved 81%
accuracy, but deteriorates when detecting drones in adverse weather conditions. In [10],
Dumitrescu et al. designed a DL-based system for drone detection by employing acoustic
signals. However, the authors did not consider malicious drones as a separate class
and the article only addressed drone detection. In [11], Digulescu et al. investigated a
radio frequency signal-based advanced signal processing model to detect the movement
of drones. The model performed relatively well in the controlled environment. In [12],
Singha et al. proposed a YOLOv4-based model for detecting drones. The model achieved
mean average precision (mAP) of 74.36%. Furthermore, in [13], the authors proposed a
DL-based detection and identiÔ¨Åcation of the drones using audio signals. The technique
achieved the highest accuracy of 85.26%; however, the model showed limited performance
in adverse weather conditions. In [14], the authors distinguished drones from birds using
the laser. The framework detected drones with less than Ô¨Åve kilograms of mass. However,
the technique was not used to detect drones with loads. In a subsequent study [15], the
authors proposed a YOLOv3 based model for detecting drones and birds. The model
performance varies with the variation in the shape of drones and the visibility of the drones.
In [16], Swinney et al. analyzed the impact of real-world interference in the classiÔ¨Åcation of
drones, using CNNs and radio frequency signals.

Figure 1. (a,b) Normal use cases of drones; (c) malicious drone intrusion in the restricted areas.

AI 2022, 3, FOR PEER REVIEW 2   Figure 1. (a,b) Normal use cases of drones; (c) malicious drone intrusion in the restricted areas. Hence, it is critical to develop an autonomous system that can efficiently detect the intrusion of malicious drones to avoid any potential damage. In that regard, machine learning (ML) and computer vision (CV) can allow us to develop automated systems that can detect malicious drones. The existing techniques in the literature usually rely on au-dios, images, videos, and radio frequency signals to detect drones. In [7], the authors pro-posed a DL-based hybrid audio and integrated visual framework for detecting malicious drones, which achieved an accuracy of 98.5% for the combined audio and visual dataset. However, the main drawback of the model was that it was limited to drone detection, and the model was unable to differentiate between drones with loads and without loads. Along similar lines, in [8], the authors proposed a mel frequency cepstral coefficient (MFCC) with a SVM-based model for detecting malicious drones; however, the perfor-mance model deteriorates when detecting amateur drones in adverse weather conditions and noisy environments. Moreover, in [9], the authors proposed a handcrafted feature extraction-based technique to detect drones using audios and images. The method achieved 81% accuracy, but deteriorates when detecting drones in adverse weather con-ditions. In [10], Dumitrescu et al. designed a DL-based system for drone detection by em-ploying acoustic signals. However, the authors did not consider malicious drones as a separate class and the article only addressed drone detection. In [11], Digulescu et al. in-vestigated a radio frequency signal-based advanced signal processing model to detect the movement of drones. The model performed relatively well in the controlled environment. In [12], Singha et al. proposed a YOLOv4-based model for detecting drones. The model achieved mean average precision (mAP) of 74.36%. Furthermore, in [13], the authors pro-posed a DL-based detection and identification of the drones using audio signals. The tech-nique achieved the highest accuracy of 85.26%; however, the model showed limited per-formance in adverse weather conditions. In [14], the authors distinguished drones from birds using the laser. The framework detected drones with less than five kilograms of mass. However, the technique was not used to detect drones with loads. In a subsequent study [15], the authors proposed a YOLOv3 based model for detecting drones and birds. The model performance varies with the variation in the shape of drones and the visibility AI 2022, 3

262

From the aforementioned discussion, it is clear that although several existing DL
models can classify and detect drones based on acoustic, radio frequency, and visual signals,
they may not be useful in challenging scenarios of distinguishing between several subject
classes, such as drones, malicious drones, birds, airplanes, helicopters, etc. Furthermore,
none of the existing ML and deep learning (DL) models address the issue of drone detection
with loads.

In order to address the aforementioned shortcomings and drawbacks, the current
article proposes a vision transformer (ViT) based framework for classifying drones, ma-
licious drones, airplanes, birds, and helicopters. The idea of ViT was introduced by [17].
We compare the proposed framework with various handcrafted feature extraction such as
histogram of oriented gradient (HOG) [18], locally encoded transform feature histogram
(LETRIST) [19], local binary pattern (LBP) [20], gray level co-occurrence matrix (GLCM) [21],
non-redundant local binary pattern (NRLBP) [22], completed joint-scale local binary pat-
tern (CJLBP) [23], local tetra pattern (LTrP) [24], and D-CNN models, such as AlexNet [25],
ShufÔ¨ÇeNet [26], ResNet-50 [27], SqueezeNet [28], MobileNet-v2 [29], Inceptionv3 [30],
GoogleNet [31], EfÔ¨ÅcientNetb0 [32], Inception-ResNet-v2 [33], DarkNet-53 [34] and Xcep-
tion [35]. We also compare the feature extractions‚Äô performance with several classiÔ¨Åers,
such as support vector machine [36,37], decision tree, k-nearest neighbors, ensemble, Naive
Bayes, multi-layer perceptron (MLP) [38,39], radial basis function (RBF) and group method
of data handling (GMDH). The comparisons demonstrate that the proposed model can
signiÔ¨Åcantly outperform existing state-of-the-art models in terms of classiÔ¨Åcation accuracy
and can be employed as a robust classiÔ¨Åcation model for malicious drones‚Äô detection. The
remainder of the paper is organized as follows: Section 2 describes the proposed methodol-
ogy, different handcrafted descriptor models, and dataset description; Section 3 deals with
the relevant Ô¨Ånding and discussion of the proposed classiÔ¨Åer. Finally, the conclusions and
prospects of the current work are discussed in Section 4.

2. Proposed Methodology

Drones have different visual characteristics, such as color, shape, load, and size. Thus,
the images are useful for distinguishing malicious drones from the other classes, such
as drones without load, aeroplanes, helicopters, and birds. The images are fed into the
handcrafted descriptors, D-CNNs, and ViT-classiÔ¨Åer. Handcrafted descriptors and D-CNNs
are used to extract features that are used to train the classiÔ¨Åer. The schematic in Figure 2
shows the Ô¨Çow diagram of the framework.

Figure 2. Flow diagram of the proposed methodology.

2.1. Handcrafted Descriptors

The images are resized to 224 √ó 224 and after that, features are extracted with the help
of HOG, LETRIST, LBP, GLCM, NRLBP, CJLBP, and LTrP. The features are stored in the
feature vectors, which are used to train ML classiÔ¨Åers.

2.2. D-CNN Models

The images are resized to the input size of each D-CNNs and after that, features are
extracted with the help of AlexNet, ShufÔ¨ÇeNet, ResNet-50, SqueezeNet, MobileNet-v2,
Inceptionv3, GoogleNet, EfÔ¨ÅcientNetb0, Inception-ResNet-v2, DarkNet-53, and Xception.
The features are saved in the feature vectors, which are used to train ML classiÔ¨Åers.

AI 2022, 3, FOR PEER REVIEW 3  of the drones. In [16], Swinney et al. analyzed the impact of real-world interference in the classification of drones, using CNNs and radio frequency signals. From the aforementioned discussion, it is clear that although several existing DL models can classify and detect drones based on acoustic, radio frequency, and visual sig-nals, they may not be useful in challenging scenarios of distinguishing between several subject classes, such as drones, malicious drones, birds, airplanes, helicopters, etc. Fur-thermore, none of the existing ML and deep learning (DL) models address the issue of drone detection with loads. In order to address the aforementioned shortcomings and drawbacks, the current article proposes a vision transformer (ViT) based framework for classifying drones, mali-cious drones, airplanes, birds, and helicopters. The idea of ViT was introduced by [17]. We compare the proposed framework with various handcrafted feature extraction such as histogram of oriented gradient (HOG) [18], locally encoded transform feature histo-gram (LETRIST) [19], local binary pattern (LBP) [20], gray level co-occurrence matrix (GLCM) [21], non-redundant local binary pattern (NRLBP) [22], completed joint-scale lo-cal binary pattern (CJLBP) [23], local tetra pattern (LTrP) [24], and D-CNN models, such as AlexNet [25], ShuffleNet [26], ResNet-50 [27], SqueezeNet [28], MobileNet-v2 [29], In-ceptionv3 [30], GoogleNet [31], EfficientNetb0 [32], Inception-ResNet-v2 [33], DarkNet-53 [34] and Xception [35]. We also compare the feature extractions‚Äô performance with several classifiers, such as support vector machine [36,37], decision tree, k-nearest neighbors, en-semble, Naive Bayes, multi-layer perceptron (MLP) [38,39], radial basis function (RBF) and group method of data handling (GMDH). The comparisons demonstrate that the pro-posed model can significantly outperform existing state-of-the-art models in terms of clas-sification accuracy and can be employed as a robust classification model for malicious drones‚Äô detection. The remainder of the paper is organized as follows: Section 2 describes the proposed methodology, different handcrafted descriptor models, and dataset descrip-tion; Section 3 deals with the relevant finding and discussion of the proposed classifier. Finally, the conclusions and prospects of the current work are discussed in Section 4. 2. Proposed Methodology Drones have different visual characteristics, such as color, shape, load, and size. Thus, the images are useful for distinguishing malicious drones from the other classes, such as drones without load, aeroplanes, helicopters, and birds. The images are fed into the handcrafted descriptors, D-CNNs, and ViT-classifier. Handcrafted descriptors and D-CNNs are used to extract features that are used to train the classifier. The schematic in Figure 2 shows the flow diagram of the framework.  Figure 2. Flow diagram of the proposed methodology. 2.1. Handcrafted Descriptors The images are resized to 224 √ó224 and after that, features are extracted with the help of HOG, LETRIST, LBP, GLCM, NRLBP, CJLBP, and LTrP. The features are stored in the feature vectors, which are used to train ML classifiers. 2.2. D-CNN Models The images are resized to the input size of each D-CNNs and after that, features are extracted with the help of AlexNet, ShuffleNet, ResNet-50, SqueezeNet, MobileNet-v2, AI 2022, 3

263

2.3. ViT-Based ClassiÔ¨Åcation

Initially, the images are resized to 224 √ó 224 and then fed into ViT. ViT splits images
into 14 √ó 14 vectors with patches of 16 √ó 16. These patch embedding vectors are followed
by adding learnable position embedding vectors. These embedded vectors are further
fed into the transformer encoder (TE), which is proposed in [40]. In TE, the embedded
vectors are divided into a query (a), key (b), and value (c) after being expanded by a fully
connected (fc) layer. Then, a, b, and c are further divided and fed to the parallel attention
heads (AH). Outputs from AHs are concatenated to form the vectors whose shape is the
same as the encoder input. The vectors go through an fc, a layer normalization, and a
multi-layer perceptron MLP block with two fc layers. TE encodes the embedding vector
and outputs a vector of the same size. The output vector of the TE is fed into the MLP head
to make the Ô¨Ånal classiÔ¨Åcation. The complete schematic diagram of the ViT is shown in
Figure 3.

Figure 3. Schematic diagram of the ViT classiÔ¨Åer.

2.4. Dataset

In the present study, a customized dataset consisting of Ô¨Åve different classes (i.e., aero-
planes, birds, drones, helicopters, and malicious drones) is utilized. The dataset is chal-
lenging due to the presence of occluded images, night images, low visibility of object
images, and adverse weather condition images. The dataset has a total of 776 images. The
aeroplane and bird classes have 105 images in each class. Similarly, the drone, helicopter,
and malicious drone classes have 200, 167, and 199 images, respectively. All the images are
resized to 224 √ó 224. The dataset is publicly available on Kaggle, and the link can be found
in the data availability section. The dataset is divided into a train set with 70% images
and a test set with 30% images. Some of the typical images from the dataset are shown in
Figure 4.

AI 2022, 3, FOR PEER REVIEW 4  Inceptionv3, GoogleNet, EfficientNetb0, Inception-ResNet-v2, DarkNet-53, and Xception. The features are saved in the feature vectors, which are used to train ML classifiers. 2.3. ViT-Based Classification Initially, the images are resized to 224 √ó224 and then fed into ViT. ViT splits im-ages into 14 √ó14 vectors with patches of 16 √ó16. These patch embedding vectors are followed by adding learnable position embedding vectors. These embedded vectors are further fed into the transformer encoder (TE), which is proposed in [40]. In TE, the em-bedded vectors are divided into a query (a), key (b), and value (c) after being expanded by a fully connected (fc) layer. Then, a, b, and c are further divided and fed to the parallel attention heads (AH). Outputs from AHs are concatenated to form the vectors whose shape is the same as the encoder input. The vectors go through an fc, a layer normaliza-tion, and a multi-layer perceptron MLP block with two fc layers. TE encodes the embed-ding vector and outputs a vector of the same size. The output vector of the TE is fed into the MLP head to make the final classification. The complete schematic diagram of the ViT is shown in Figure 3.  Figure 3. Schematic diagram of the ViT classifier. 2.4. Dataset In the present study, a customized dataset consisting of five different classes (i.e., aero-planes, birds, drones, helicopters, and malicious drones) is utilized. The dataset is challenging due to the presence of occluded images, night images, low visibility of object images, and adverse weather condition images. The dataset has a total of 776 images. The aeroplane and bird classes have 105 images in each class. Similarly, the drone, helicopter, and malicious drone classes have 200, 167, and 199 images, respectively. All the images are resized to 224√ó224. The dataset is publicly available on Kaggle, and the link can be found in the data availability section. The dataset is divided into a train set with 70% im-ages and a test set with 30% images. Some of the typical images from the dataset are shown in Figure 4. AI 2022, 3

264

Figure 4. Sample images from the custom dataset for Ô¨Åve different classes: (a) aeroplane; (b) bird;
(c) drone; (d) helicopter; and (e) malicious drone.

3. Results

In order to evaluate the performance of the proposed classiÔ¨Åer, various performance
metrics, including accuracy, speciÔ¨Åcity, sensitivity, and F1 ‚àí score are considered. The
accuracy of the classiÔ¨Åer can be obtained as follows:

Accuracy =

tp + tn
tp + tn + f p + fn

(1)

where, in Equation (1), tn and tp denote true negative and true positive, respectively, while
fn and f p represent false negative and false positive, respectively. The accuracy of the
classiÔ¨Åer indicates the ability to distinguish malicious drone classes correctly. Sensitivity
(se) is the proportion of actual positives that are correctly predicted as positives and is
determined as

se =

tp
tp + fn

(2)

Precision or speciÔ¨Åcity (cid:0)sp

(cid:1) is the proportion of actual positives that are correctly

predicted as negatives and is calculated as follows:

sp =

tn
tn + f p

(3)

AI 2022, 3, FOR PEER REVIEW 5                           (a) (b) (c) (d) (e) Figure 4. Sample images from the custom dataset for five different classes: (a) aeroplane; (b) bird; (c) drone; (d) helicopter; and (e) malicious drone. 3. Results In order to evaluate the performance of the proposed classifier, various performance metrics, including accuracy, specificity, sensitivity, and ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí are considered. The accuracy of the classifier can be obtained as follows: Accuracy=ùë°(cid:3043)+ùë°(cid:3041)ùë°(cid:3043)+ùë°(cid:3041)+ùëì(cid:3043)+ùëì(cid:3041) (1)where, in Equation (1), ùë°(cid:3041) and ùë°(cid:3043) denote true negative and true positive, respectively, while ùëì(cid:3041) and ùëì(cid:3043) represent false negative and false positive, respectively. The accuracy of the classifier indicates the ability to distinguish malicious drone classes correctly. Sensi-tivity (ùë†(cid:3032)) is the proportion of actual positives that are correctly predicted as positives and is determined as ùë†(cid:3032)=ùë°(cid:3043)ùë°(cid:3043)+ùëì(cid:3041) (2)Precision or specificity (ùë†(cid:3043)) is the proportion of actual positives that are correctly predicted as negatives and is calculated as follows: ùë†(cid:3043)=ùë°(cid:3041)ùë°(cid:3041)+ùëì(cid:3043) (3)AI 2022, 3

265

From the definition of se and sp in Equations (2) and (3), the F1 ‚àí score can be obtained as

F1 ‚àí score = 2 √ó

(cid:35)

(cid:34) (cid:0)se ‚àó sp
(cid:1)
(cid:1)
(cid:0)se + sp

(4)

Additionally, Cohen‚Äôs kappa (Œ∫) is considered to further evaluate the performance of

the proposed model, which can be calculated as

(cid:34)

Œ∫ = 2 √ó

(cid:0)tp + f p

(cid:0)tp¬∑tn ‚àí f p¬∑ fn

(cid:1)

(cid:35)

(cid:1)¬∑(cid:0) f p + tn

(cid:1) + (cid:0)tp + fn

(cid:1)¬∑( fn + tn)

(5)

The experiments are conducted on the local system with 12 GB RAM and Tesla T4

GPU. The model complexity and hyperparameters of the model are shown in Table 1.

Table 1. Model complexity and hyperparameters.

Parameter

Trainable Parameters
Model Parameters size
Learning rate
Optimizer
Mini Batch Size

Value

85.8 M
171.605 MB
2 √ó 10‚àí5
Adam
8

From the classiÔ¨Åcation result, it is found that the proposed ViT classiÔ¨Åer has achieved
98.28% overall accuracy. The accuracy values for aeroplanes, birds, and helicopters are
100%, 100%, and 100%, respectively, indicating excellent robustness of the model for these
classes. However, the accuracy values for the drone and malicious drone classes slightly
drop to 96.8% and 96.8%, respectively. The confusion matrix of the ViT classiÔ¨Åer is shown
in Figure 5.

Figure 5. Confusion matrix of the ViT classiÔ¨Åer.

AI 2022, 3, FOR PEER REVIEW 6  From the definition of ùë†(cid:3032) and ùë†(cid:3043) in Equations (2) and (3), the ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí can be ob-tained as ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí=2√ó(cid:4680)(ùë†(cid:3032)‚àóùë†(cid:3043))(ùë†(cid:3032)+ùë†(cid:3043))(cid:4681) (4)Additionally, Cohen‚Äôs kappa (ùúÖ) is considered to further evaluate the performance of the proposed model, which can be calculated as ùúÖ=2√ó(cid:4680)(ùë°(cid:3043).ùë°(cid:3041)‚àíùëì(cid:3043).ùëì(cid:3041))(cid:3435)ùë°(cid:3043)+ùëì(cid:3043)(cid:3439).(cid:3435)ùëì(cid:3043)+ùë°(cid:3041)(cid:3439)+(cid:3435)ùë°(cid:3043)+ùëì(cid:3041)(cid:3439).(ùëì(cid:3041)+ùë°(cid:3041))(cid:4681) (5)The experiments are conducted on the local system with 12 GB RAM and Tesla T4 GPU. The model complexity and hyperparameters of the model are shown in Table 1. Table 1. Model complexity and hyperparameters. Parameter Value Trainable Parameters 85.8 M Model Parameters size 171.605 MB Learning rate 2 √ó 10‚àí5 Optimizer Adam Mini Batch Size 8 From the classification result, it is found that the proposed ViT classifier has achieved 98.28% overall accuracy. The accuracy values for aeroplanes, birds, and helicopters are 100%, 100%, and 100%, respectively, indicating excellent robustness of the model for these classes. However, the accuracy values for the drone and malicious drone classes slightly drop to 96.8% and 96.8%, respectively. The confusion matrix of the ViT classifier is shown in Figure 5.  Figure 5. Confusion matrix of the ViT classifier. The ViT classifier achieves the overall ùë†(cid:3032), ùë†(cid:3043), ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí, and ùúÖ values of 99.00%, 99.00%, 99.00%, and 99.00%, respectively. The ùë†(cid:3032), ùë†(cid:3043) and ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí of aeroplane, bird, and helicopter classes are 100%, 100%, and 100% respectively. The ùë†(cid:3032), ùë†(cid:3043), and ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí AI 2022, 3

266

The ViT classiÔ¨Åer achieves the overall se, sp, F1 ‚àí score, and Œ∫ values of 99.00%, 99.00%,
99.00%, and 99.00%, respectively. The se, sp and F1 ‚àí score of aeroplane, bird, and helicopter
classes are 100%, 100%, and 100% respectively. The se, sp, and F1 ‚àí score for drone and
malicious drone classes are 97.0%, 97.0%, and 97.0%, respectively. Figure 6 shows the
comparison bar chart of various classiÔ¨Åcation metrics obtained from the ViT classiÔ¨Åer for
different classes.

Figure 6. The classiÔ¨Åcation performance metrics for different individual class and overall classes of
the ViT classiÔ¨Åer.

This section reports the performance comparison of various handcrafted descriptors
considering different classiÔ¨Åers. The accuracy of the HOG, LETRIST, LBP, GLCM, NRLBP,
CJLBP, and LTrP with different classiÔ¨Åers such as SVM with linear kernel, kNN, DT,
Ensemble, NB, MLP, RBF, and GMDH are shown in Table 2.

Analyzing Table 2, it is evident that the performance of handcrafted descriptors is quite
low compared to ViT classiÔ¨Åer, as the highest accuracy is 78.90% using HOG and ensemble
classiÔ¨Åer. The accuracy of HOG with the SVM classiÔ¨Åer is 76.70% whereas, with kNN, NB
and DT, it is 37.90%, 57.30%, and 58.60%, respectively. Similarly, Table 3 shows the test
accuracy of the AlexNet, ShufÔ¨ÇeNet, ResNet-50, SqueezeNet, MobileNet-v2, Inceptionv3,
GoogleNet, EfÔ¨ÅcientNetb0, Inception-ResNet-v2, DarkNet-53, and Xception models with
different classiÔ¨Åers. All the D-CNNs are trained with 1000 epochs and the best number of
the epochs are achieved by monitoring the validation accuracy of the models and adding
early stopping.

AI 2022, 3, FOR PEER REVIEW 7  for drone and malicious drone classes are 97.0%, 97.0%, and 97.0%, respectively. Figure 6 shows the comparison bar chart of various classification metrics obtained from the ViT classifier for different classes.  Figure 6. The classification performance metrics for different individual class and overall classes of the ViT classifier. This section reports the performance comparison of various handcrafted descriptors considering different classifiers. The accuracy of the HOG, LETRIST, LBP, GLCM, NRLBP, CJLBP, and LTrP with different classifiers such as SVM with linear kernel, kNN, DT, En-semble, NB, MLP, RBF, and GMDH are shown in Table 2. Table 2. Performance comparison of various handcrafted descriptors considering different classifi-ers. Descriptor Classifier Accuracy HOG SVM (Linear Kernel) 1 76.70% kNN 2 37.90% DT 3 58.60% NB 4 57.30% Ensemble 78.90% MLP 70.50% RBF 75.60% GMDH 74.50% LETRIST SVM (Linear Kernel) 1 31.90% kNN 2 39.70% DT 3 36.60% NB 4 43.50% Ensemble 52.20% MLP 30.90% RBF 32.30% GMDH 30.40% LBP SVM (Linear Kernel) 1 45.30% kNN 2 38.40% DT 3 34.90% AI 2022, 3

267

Table 2. Performance comparison of various handcrafted descriptors considering different classiÔ¨Åers.

Descriptor

ClassiÔ¨Åer

Accuracy

HOG

LETRIST

LBP

GLCM

NRLBP

CJLBP

LTrP

SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM (Linear Kernel) 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH

76.70%
37.90%
58.60%
57.30%
78.90%
70.50%
75.60%
74.50%

31.90%
39.70%
36.60%
43.50%
52.20%
30.90%
32.30%
30.40%

45.30%
38.40%
34.90%
39.70%
45.70%
39.10%
44.20%
43.10%

49.60%
36.60%
39.20%
34.50%
44.40%
43.40%
48.50%
47.40%

28.00%
16.80%
30.60%
30.60%
22.00%
27.00%
26.00%

36.20%
30.20%
38.40%
36.60%
50.90%
30.00%
35.10%
34.00%

29.70%
34.10%
37.90%
44.80%
47.80%
23.50%
28.60%
27.50%

1 SVM = support vector machine, 2 kNN = k nearest neighbor, 3 DT = decision tree, 4 NB = na√Øve Bayes.

AI 2022, 3

268

Table 3. Performance values in terms of accuracy obtained from different D-CNN models.

D-CNN Model

AlexNet

ShufÔ¨ÇeNet

ResNet-50

SqueezeNet

MobileNet-v2

Inceptionv3

ClassiÔ¨Åer

SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH

Accuracy

88.80%
75.90%
59.50%
71.10%
83.30%
82.60%
87.70%
86.60%

86.20%
77.60%
63.80%
76.30%
86.20%
80.00%
85.10%
84.00%

89.20%
77.20%
73.70%
72.80%
86.60%
83.00%
88.10%
87.00%

61.60%
64.20%
66.40%
63.80%
82.80%
55.40%
60.50%
59.40%

91.80%
84.50%
62.90%
83.60%
85.30%
85.60%
90.70%
89.60%

90.90%
88.40%
70.70%
85.30%
88.40%
84.70%
89.80%
88.70%

AI 2022, 3

269

Table 3. Cont.

D-CNN Model

GoogleNet

EfÔ¨ÅcientNetb0

Inception-ResNet-v2

DarkNet-53

Xception

ClassiÔ¨Åer

SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH
SVM 1
kNN 2
DT 3
NB 4
Ensemble
MLP
RBF
GMDH

Proposed

ViT classiÔ¨Åer

Accuracy

87.90%
82.30%
64.20%
84.50%
87.50%
83.70%
86.80%
85.70%

92.20%
84.50%
66.40%
86.20%
89.20%
86.00%
91.10%
90.00%

91.80%
87.90%
72.00%
80.20%
89.70%
85.60%
90.70%
89.60%

68.50%
62.50%
75.00%
74.60%
91.40%
62.30%
67.40%
66.30%

93.50%
87.90%
72.40%
87.50%
88.80%
87.30%
92.40%
91.30%

98.28%

1 SVM = support vector machine, 2 kNN = k nearest neighbor, 3 DT = decision tree, 4 NB = na√Øve Bayes.

The results in Table 3 indicate that the performance of the D-CNN models is better
than handcrafted descriptors. However, the highest accuracy of 93.50% is achieved by
Xception with multiclass SVM. The accuracy values for the Xception with kNN, DT, NB,
and Ensemble are 87.90%, 72.40%, 87.50%, and 88.80%, respectively. The highest accuracy
achieved by AlexNet is 88.80% with SVM. Similarly, ResNet-50 achieves the maximum
accuracy of 89.20% with SVM. ShufÔ¨ÇeNet achieves the highest accuracy of 86.20% with SVM
and ensemble. SqueezeNet achieves the highest accuracy of the 82.80% with the ensemble.
MobileNet-v2, Inceptionv3, GoogleNet, EfÔ¨ÅcientNetb0 and Inception-ResNet-v2 has 91.80%,
90.90%, 87.90%, 92.20% and 91.80% accuracy with the SVM classiÔ¨Åer, respectively. However,
DarkNet-53 achieves the highest accuracy of 91.40% with the ensemble. The proposed
framework with ViT classiÔ¨Åer achieves an accuracy of 98.28%, which is a 4.78% increase

AI 2022, 3

270

in accuracy compared to Xception with SVM. The comparisons demonstrate that the
proposed model can signiÔ¨Åcantly outperform existing D-CNN models by achieving the
highest classiÔ¨Åcation accuracy.

We also visualized the hot maps of the Grad CAM to visualize the portion of the image,
which helps in classiÔ¨Åcation of the images with 10% to 90% background. The visual results
are shown in Figure 7.

Figure 7. Hot map visualization of malicious drone images with 10% to 90% background using Grad CAM.

From Figure 7, it can be observed that when the load is near the drone, even in the 90%
background images, it contributes to the classiÔ¨Åcation. However, when the load is tied with
string or relatively far away from the drone body, then only the drone contributes to the
classiÔ¨Åcation. From the performance comparison, it is evident that the proposed framework
can be employed as a robust and efÔ¨Åcient classiÔ¨Åcation model for malicious drone detection.
The current framework can be extended for the image compression [41], classiÔ¨Åcation,
and other computer vision tasks, such as object detection [42‚Äì44], and motor imagery
classiÔ¨Åcation in the brain‚Äìcomputer interface (BCI) [45‚Äì47]. The work can further be
extended to classify malicious drones using selected features with nature and bio inspired
algorithms [48‚Äì50], such as particle swarm optimization (PSO), genetic algorithm (GA),
artiÔ¨Åcial bee colony (ABC), etc.

4. Conclusions

Drones are widely used due to their numerous applications. However, malicious
drones which carry harmful material can cause destruction and bomb blasts. Thus, it is
critical to distinguish between malicious drones and other Ô¨Çying objects. In this article,
several ML and DL techniques are analyzed, which reveal that the performance of the
handcrafted descriptors with ML classiÔ¨Åers is relatively low. Furthermore, the performance
of various D-CNN ML classiÔ¨Åers is also evaluated. Our study indicates that the highest
accuracy achieved by D-CNN models is 93.50%. However, the overall classiÔ¨Åcation accuracy
of the ViT classiÔ¨Åer is 98.3%, which is the highest among all models. The ViT classiÔ¨Åer
achieves the overall recall, precision, and F1 ‚àí score of 99.0%, 99.0%, 99.0%, and 99.0%,
respectively. The precision, recall, F1 ‚àí score, and Cohen‚Äôs kappa for malicious drone class
are 97.0%, 97.0%, 97.0%, and 97.0%, respectively. The current study illustrates that the
proposed ViT-based approach can help to classify malicious drones more efÔ¨Åciently than
state-of-the-art D-CNN models. Training with a large dataset can further enhance the
performance of the ViT-based framework. Nevertheless, the current framework can also

AI 2022, 3, FOR PEER REVIEW 10  RBF 91.10% GMDH 90.00% Inception-ResNet-v2 SVM 1 91.80% kNN 2 87.90% DT 3 72.00% NB 4 80.20% Ensemble 89.70% MLP 85.60% RBF 90.70% GMDH 89.60% DarkNet-53 SVM 1 68.50% kNN 2 62.50% DT 3 75.00% NB 4 74.60% Ensemble 91.40% MLP 62.30% RBF 67.40% GMDH 66.30% Xception SVM 1 93.50% kNN 2 87.90% DT 3 72.40% NB 4 87.50% Ensemble 88.80% MLP 87.30% RBF 92.40% GMDH 91.30% Proposed ViT classifier 98.28% 1 SVM = support vector machine, 2 kNN = k nearest neighbor, 3 DT = decision tree, 4 NB = na√Øve Bayes. The results in Table 3 indicate that the performance of the D-CNN models is better than handcrafted descriptors. However, the highest accuracy of 93.50% is achieved by Xception with multiclass SVM. The accuracy values for the Xception with kNN, DT, NB, and Ensemble are 87.90%, 72.40%, 87.50%, and 88.80%, respectively. The highest accuracy achieved by AlexNet is 88.80% with SVM. Similarly, ResNet-50 achieves the maximum accuracy of 89.20% with SVM. ShuffleNet achieves the highest accuracy of 86.20% with SVM and ensemble. SqueezeNet achieves the highest accuracy of the 82.80% with the en-semble. MobileNet-v2, Inceptionv3, GoogleNet, EfficientNetb0 and Inception-ResNet-v2 has 91.80%, 90.90%, 87.90%, 92.20% and 91.80% accuracy with the SVM classifier, respec-tively. However, DarkNet-53 achieves the highest accuracy of 91.40% with the ensemble. The proposed framework with ViT classifier achieves an accuracy of 98.28%, which is a 4.78% increase in accuracy compared to Xception with SVM. The comparisons demon-strate that the proposed model can significantly outperform existing D-CNN models by achieving the highest classification accuracy. We also visualized the hot maps of the Grad CAM to visualize the portion of the image, which helps in classification of the images with 10% to 90% background. The visual results are shown in Figure 7.      AI 2022, 3, FOR PEER REVIEW 11       Figure 7. Hot map visualization of malicious drone images with 10% to 90% background using Grad CAM. From Figure 7, it can be observed that when the load is near the drone, even in the 90% background images, it contributes to the classification. However, when the load is tied with string or relatively far away from the drone body, then only the drone contrib-utes to the classification. From the performance comparison, it is evident that the pro-posed framework can be employed as a robust and efficient classification model for mali-cious drone detection. The current framework can be extended for the image compression [41], classification, and other computer vision tasks, such as object detection [42‚Äì44], and motor imagery classification in the brain‚Äìcomputer interface (BCI) [45‚Äì47]. The work can further be extended to classify malicious drones using selected features with nature and bio inspired algorithms [48‚Äì50], such as particle swarm optimization (PSO), genetic algo-rithm (GA), artificial bee colony (ABC), etc. 4. Conclusions Drones are widely used due to their numerous applications. However, malicious drones which carry harmful material can cause destruction and bomb blasts. Thus, it is critical to distinguish between malicious drones and other flying objects. In this article, several ML and DL techniques are analyzed, which reveal that the performance of the handcrafted descriptors with ML classifiers is relatively low. Furthermore, the perfor-mance of various D-CNN ML classifiers is also evaluated. Our study indicates that the highest accuracy achieved by D-CNN models is 93.50%. However, the overall classifica-tion accuracy of the ViT classifier is 98.3%, which is the highest among all models. The ViT classifier achieves the overall recall, precision, and ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí of 99.0%, 99.0%, 99.0%, and 99.0%, respectively. The precision, recall, ùêπ(cid:2869)‚àíùë†ùëêùëúùëüùëí, and Cohen‚Äôs kappa for mali-cious drone class are 97.0%, 97.0%, 97.0%, and 97.0%, respectively. The current study il-lustrates that the proposed ViT-based approach can help to classify malicious drones more efficiently than state-of-the-art D-CNN models. Training with a large dataset can further enhance the performance of the ViT-based framework. Nevertheless, the current frame-work can also be extended to various classification and computer vision tasks, such as object detection, motor imagery classification in the brain‚Äìcomputer interface, etc. Author Contributions: Conceptualization, S.J. and M.S.A.; methodology, S.J.; software, S.J.; valida-tion, S.J., M.S.A. and A.M.R.; formal analysis, S.J.; writing‚Äîoriginal draft preparation, S.J. and M.S.A.; writing‚Äîreview and editing, A.M.R.; visualization, S.J.; supervision, A.M.R. All authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Dataset link: https://www.kaggle.com/sonainjamil/malicious-drones (accessed on 28 February 2022). Conflicts of Interest: The authors declare no conflict of interest.   AI 2022, 3

271

be extended to various classiÔ¨Åcation and computer vision tasks, such as object detection,
motor imagery classiÔ¨Åcation in the brain‚Äìcomputer interface, etc.

Author Contributions: Conceptualization, S.J. and M.S.A.; methodology, S.J.; software, S.J.; valida-
tion, S.J., M.S.A. and A.M.R.; formal analysis, S.J.; writing‚Äîoriginal draft preparation, S.J. and M.S.A.;
writing‚Äîreview and editing, A.M.R.; visualization, S.J.; supervision, A.M.R. All authors have read
and agreed to the published version of the manuscript.

Funding: This research received no external funding.

Institutional Review Board Statement: Not applicable.

Informed Consent Statement: Not applicable.

Data Availability Statement: Dataset link: https://www.kaggle.com/sonainjamil/malicious-drones
(accessed on 28 February 2022).

ConÔ¨Çicts of Interest: The authors declare no conÔ¨Çict of interest.

References

1.

2.

3.

4.

5.

6.

7.

8.

9.

Ayamga, M.; Tekinerdogan, B.; Kassahun, A. Exploring the Challenges Posed by Regulations for the Use of Drones in Agriculture
in the African Context. Land 2021, 10, 164. [CrossRef]
Cancela, J.J.; Gonz√°lez, X.P.; Vilanova, M.; Mir√°s-Avalos, J.M. Water Management Using Drones and Satellites in Agriculture.
Water 2019, 11, 874. [CrossRef]
Hwang, J.; Kim, I.; Gulzar, M.A. Understanding the Eco-Friendly Role of Drone Food Delivery Services: Deepening the Theory of
Planned Behavior. Sustainability 2020, 12, 1440. [CrossRef]
Dal Sasso, S.F.; Pizarro, A.; Manfreda, S. Recent Advancements and Perspectives in UAS-Based Image Velocimetry. Drones 2021,
5, 81. [CrossRef]
Amponis, G.; Lagkas, T.; Zevgara, M.; Katsikas, G.; Xirofotos, T.; Moscholios, I.; Sarigiannidis, P. Drones in B5G/6G Networks as
Flying Base Stations. Drones 2022, 6, 39. [CrossRef]
Verdiesen, I.; Aler Tubella, A.; Dignum, V. Integrating Comprehensive Human Oversight in Drone Deployment: A Conceptual
Framework Applied to the Case of Military Surveillance Drones. Information 2021, 12, 385. [CrossRef]
Jamil, S.; Fawad; Rahman, M.; Ullah, A.; Badnava, S.; Forsat, M.; Mirjavadi, S.S. Malicious UAV Detection Using Integrated Audio
and Visual Features for Public Safety Applications. Sensors 2020, 20, 3923. [CrossRef] [PubMed]
Anwar, M.Z.; Kaleem, Z.; Jamalipour, A. Machine Learning Inspired Sound-Based Amateur Drone Detection for Public Safety
Applications. IEEE Trans. Veh. Technol. 2019, 68, 2526‚Äì2534. [CrossRef]
Liu, H.; Wei, Z.; Chen, Y.; Pan, J.; Lin, L.; Ren, Y. Drone detection based on an audio-assisted camera array. In Proceedings
of the 2017 IEEE Third International Conference on Multimedia Big Data (BigMM), Laguna Hills, CA, USA, 19‚Äì21 April
2017; pp. 402‚Äì406.

10. Dumitrescu, C.; Minea, M.; Costea, I.M.; Cosmin Chiva, I.; Semenescu, A. Development of an Acoustic System for UAV Detection.

Sensors 2020, 20, 4870. [CrossRef]

11. Digulescu, A.; Despina-Stoian, C.; StƒÉnescu, D.; Popescu, F.; Enache, F.; Ioana, C.; RƒÉdoi, E.; R√Æncu, I.; S, erbƒÉnescu, A. New
Approach of UAV Movement Detection and Characterization Using Advanced Signal Processing Methods Based on UWB Sensing.
Sensors 2020, 20, 5904. [CrossRef] [PubMed]
Singha, S.; Aydin, B. Automated Drone Detection Using YOLOv4. Drones 2021, 5, 95. [CrossRef]

12.
13. Al-Emadi, S.; Al-Ali, A.; Al-Ali, A. Audio-Based Drone Detection and IdentiÔ¨Åcation Using Deep Learning Techniques with

Dataset Enhancement through Generative Adversarial Networks. Sensors 2021, 21, 4953. [CrossRef] [PubMed]

14. Wojtanowski, J.; Zygmunt, M.; Drozd, T.; Jakubaszek, M.; ÀôZyczkowski, M.; Muzal, M. Distinguishing Drones from Birds in a UAV

Searching Laser Scanner Based on Echo Depolarization Measurement. Sensors 2021, 21, 5597. [CrossRef]

15. Coluccia, A.; Fascista, A.; Schumann, A.; Sommer, L.; Dimou, A.; Zarpalas, D.; M√©ndez, M.; de la Iglesia, D.; Gonz√°lez, I.; Mercier,
J.-P.; et al. Drone vs. Bird Detection: Deep Learning Algorithms and Results from a Grand Challenge. Sensors 2021, 21, 2824.
[CrossRef] [PubMed]
Swinney, C.J.; Woods, J.C. The Effect of Real-World Interference on CNN Feature Extraction and Machine Learning ClassiÔ¨Åcation
of Unmanned Aerial Systems. Aerospace 2021, 8, 179. [CrossRef]

16.

17. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Houlsby, N. An image is worth 16 √ó 16

words: Transformers for image recognition at scale. arXiv 2020, arXiv:2010.11929.

18. Patel, C.I.; Labana, D.; Pandya, S.; Modi, K.; Ghayvat, H.; Awais, M. Histogram of Oriented Gradient-Based Fusion of Features for

19.

Human Action Recognition in Action Video Sequences. Sensors 2020, 20, 7299. [CrossRef] [PubMed]
Song, T.; Li, H.; Meng, F.; Wu, Q.; Cai, J. LETRIST: Locally encoded transform feature histogram for rotation-invariant texture
classiÔ¨Åcation. IEEE Trans. Circuits Syst. Video Technol. 2017, 28, 1565‚Äì1579. [CrossRef]

AI 2022, 3

272

20. Yasmin, S.; Pathan, R.K.; Biswas, M.; Khandaker, M.U.; Faruque, M.R.I. Development of a Robust Multi-Scale Featured Local

21.

Binary Pattern for Improved Facial Expression Recognition. Sensors 2020, 20, 5391. [CrossRef] [PubMed]
Fanizzi, A.; Basile, T.M.; Losurdo, L.; Bellotti, R.; Bottigli, U.; Campobasso, F.; Didonna, V.; Fausto, A.; Massafra, R.; Tagli-
aÔ¨Åco, A.; et al. Ensemble Discrete Wavelet Transform and Gray-Level Co-Occurrence Matrix for MicrocalciÔ¨Åcation Cluster
ClassiÔ¨Åcation in Digital Mammography. Appl. Sci. 2019, 9, 5388. [CrossRef]

22. Nguyen, D.T.; Zong, Z.; Ogunbona, P.; Li, W. Object detection using non-redundant local binary patterns. In Proceedings of the

17th IEEE International Conference on Image Processing, Hong Kong, China, 26‚Äì29 September 2010; pp. 4609‚Äì4612.

23. Wu, X.; Sun, J. Joint-scale LBP: A new feature descriptor for texture classiÔ¨Åcation. Vis. Comput. 2017, 33, 317‚Äì329. [CrossRef]
24. Murala, S.; Maheshwari, R.P.; Balasubramanian, R. Local Tetra Patterns: A New Feature Descriptor for Content-Based Image

Retrieval. IEEE Trans. Image Process. 2012, 21, 2874‚Äì2886. [CrossRef]

25. Minhas, R.A.; Javed, A.; Irtaza, A.; Mahmood, M.T.; Joo, Y.B. Shot ClassiÔ¨Åcation of Field Sports Videos Using AlexNet Convolu-

tional Neural Network. Appl. Sci. 2019, 9, 483. [CrossRef]

26. Liu, G.; Zhang, C.; Xu, Q.; Cheng, R.; Song, Y.; Yuan, X.; Sun, J. I3D-ShufÔ¨Çenet Based Human Action Recognition. Algorithms 2020,

27.

13, 301. [CrossRef]
Fulton, L.V.; Dolezel, D.; Harrop, J.; Yan, Y.; Fulton, C.P. ClassiÔ¨Åcation of Alzheimer‚Äôs Disease with and without Imagery Using
Gradient Boosted Machines and ResNet-50. Brain Sci. 2019, 9, 212. [CrossRef] [PubMed]

28. Wang, A.; Wang, M.; Jiang, K.; Cao, M.; Iwahori, Y. A Dual Neural Architecture Combined SqueezeNet with OctConv for LiDAR

Data ClassiÔ¨Åcation. Sensors 2019, 19, 4927. [CrossRef] [PubMed]

29. Li, W.; Liu, K. ConÔ¨Ådence-Aware Object Detection Based on MobileNetv2 for Autonomous Driving. Sensors 2021, 21, 2380.

30.

[CrossRef]
Sun, X.; Li, Z.; Zhu, T.; Ni, C. Four-Dimension Deep Learning Method for Flower Quality Grading with Depth Information.
Electronics 2021, 10, 2353. [CrossRef]

31. Lee, Y.; Nam, S. Performance Comparisons of AlexNet and GoogLeNet in Cell Growth Inhibition IC50 Prediction. Int. J. Mol. Sci.

32.

2021, 22, 7721. [CrossRef] [PubMed]
Jamil, S.; Rahman, M.; Haider, A. Bag of Features (BoF) Based Deep Learning Framework for Bleached Corals Detection. Big Data
Cogn. Comput. 2021, 5, 53. [CrossRef]

33. Ananda, A.; Ngan, K.H.; Karaba Àòg, C.; Ter-Sarkisov, A.; Alonso, E.; Reyes-Aldasoro, C.C. ClassiÔ¨Åcation and Visualisation of
Normal and Abnormal Radiographs; A Comparison between Eleven Convolutional Neural Network Architectures. Sensors 2021,
21, 5381. [CrossRef]

34. Demertzis, K.; Tsiknas, K.; Takezis, D.; Skianis, C.; Iliadis, L. Darknet TrafÔ¨Åc Big-Data Analysis and Network Management for
Real-Time Automating of the Malicious Intent Detection Process by a Weight Agnostic Neural Networks Framework. Electronics
2021, 10, 781. [CrossRef]

35. Chao, X.; Hu, X.; Feng, J.; Zhang, Z.; Wang, M.; He, D. Construction of Apple Leaf Diseases IdentiÔ¨Åcation Networks Based on

Xception Fused by SE Module. Appl. Sci. 2021, 11, 4614. [CrossRef]

36. Guo, Y.; Fu, Y.; Hao, F.; Zhang, X.; Wu, W.; Jin, X.; Bryant, C.R.; Senthilnath, J. Integrated phenology and climate in rice yields

37.

prediction using machine learning methods. Ecol. Indic. 2021, 120, 106935. [CrossRef]
Joachims, T. 11 Making Large-Scale Support Vector Machine Learning Practical. In Advances in Kernel Methods: Support Vector
Learning; The MIT Press: Cambridge, MA, USA, 1999; p. 169.

38. Roshani, M.; Phan, G.T.T.; Ali, P.J.M.; Roshani, G.H.; Hanus, R.; Duong, T.; Corniani, E.; Nazemi, E.; Kalmoun, E.M. Evaluation of
Ô¨Çow pattern recognition and void fraction measurement in two phase Ô¨Çow independent of oil pipeline‚Äôs scale layer thickness.
Alex. Eng. J. 2021, 6, 1955‚Äì1966. [CrossRef]
Sattari, M.A.; Roshani, G.H.; Hanus, R.; Nazemi, E. Applicability of time-domain feature extraction methods and artiÔ¨Åcial
intelligence in two-phase Ô¨Çow meters based on gamma-ray absorption technique. Measurement 2021, 168, 108474. [CrossRef]

39.

40. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, ≈Å.; Polosukhin, I. Attention is all you need. In
Proceedings of the Advances in Neural Information Processing Systems, Long Beach, CA, USA, 4‚Äì9 December 2017; p. 30.
Jamil, S.; Piran, M.J.; Rahman, M. Learning-Driven Lossy Image Compression; A Comprehensive Survey.
arXiv:2201.09240.

arXiv 2022,

41.

42. Roy, A.M.; Bhaduri, J. A Deep Learning Enabled Multi-Class Plant Disease Detection Model Based on Computer Vision. AI 2021,

2, 413‚Äì428. [CrossRef]

43. Roy, A.M.; Bhaduri, J. Real-time growth stage detection model for high degree of occultation using DenseNet-fused YOLOv4.

Comput. Electron. Agric. 2022, 193, 106694. [CrossRef]

44. Roy, A.M.; Bose, R.; Bhaduri, J. A fast accurate Ô¨Åne-grain object detection model based on YOLOv4 deep neural network. Neural

Comput. Appl. 2022, 34, 3895‚Äì3921.

45. Roy, A.M. An efÔ¨Åcient multi-scale CNN model with intrinsic feature integration for motor imagery EEG subject classiÔ¨Åcation in

brain-machine interfaces. Biomed. Signal Process. Control 2022, 74, 103496. [CrossRef]

46. Roy, A.M. A multi-scale fusion CNN model based on adaptive transfer learning for multi-class MI-classiÔ¨Åcation in BCI system.

47.

BioRxiv 2022. [CrossRef]
Jamil, S.; Rahman, M. A Novel Deep-Learning-Based Framework for the ClassiÔ¨Åcation of Cardiac Arrhythmia. J. Imaging 2022,
8, 70. [CrossRef]

AI 2022, 3

273

48.

Jamil, S.; Rahman, M.; Tanveer, J.; Haider, A. Energy EfÔ¨Åciency and Throughput Maximization Using Millimeter Waves‚Äì
Microwaves HetNets. Electronics 2022, 11, 474. [CrossRef]

49. Too, J.; Abdullah, A.R.; Mohd Saad, N.; Tee, W. EMG Feature Selection and ClassiÔ¨Åcation Using a Pbest-Guide Binary Particle

50.

Swarm Optimization. Computation 2019, 7, 12. [CrossRef]
Jamil, S.; Rahman, M.; Abbas, M.S.; Fawad. Resource Allocation Using ReconÔ¨Ågurable Intelligent Surface (RIS)-Assisted Wireless
Networks in Industry 5.0 Scenario. Telecom 2022, 3, 163‚Äì173. [CrossRef]

