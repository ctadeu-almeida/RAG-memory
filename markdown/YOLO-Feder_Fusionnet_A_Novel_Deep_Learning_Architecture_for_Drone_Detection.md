5
5
3
7
4
6
0
1
.
4
2
0
2
.
7
8
2
1
5
P
I
C

I
/
9
0
1
1
.
0
1

:
I

O
D

|

E
E
E
I

4
2
0
2
©
0
0
.
1
3
$
/
4
2
/
9
-
9
3
9
4
-
3
0
5
3
-
8
-
9
7
9

|

)
P
I
C

I
(

g
n
i
s
s
e
c
o
r
P
e
g
a
m

I

n
o

e
c
n
e
r
e
f
n
o
C

l
a
n
o
i
t
a
n
r
e
t
n
I
E
E
E
I

4
2
0
2

979-8-3503-4939-9/24/$31.00 ©2024 IEEE

2299

ICIP 2024

Authorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

YOLO-FEDERFUSIONNET:ANOVELDEEPLEARNINGARCHITECTUREFORDRONEDETECTIONTamaraR.Lenhard1,2,∗,AndreasWeinmann2,StefanJ¨ager1andTobiasKoch11InstitutefortheProtectionofTerrestrialInfrastructures,GermanAerospaceCenter(DLR),SanktAugustin,Germany2WorkingGroupAlgorithmsforComputerVision,ImagingandDataAnalysis,UniversityofAppliedSciencesDarmstadt,Darmstadt,GermanyABSTRACTPredominantmethodsforimage-baseddronedetectionfrequentlyrelyonemployinggenericobjectdetectionalgorithmslikeYOLOv5.Whileproficientinidentifyingdronesagainsthomogeneousback-grounds,thesealgorithmsoftenstruggleincomplex,highlytexturedenvironments.Insuchscenarios,dronesseamlesslyintegrateintothebackground,creatingcamouflageeffectsthatadverselyaffectthedetectionquality.Toaddressthisissue,weintroduceanoveldeeplearningarchitecturecalledYOLO-FEDERFusionNet.Unlikecon-ventionalapproaches,YOLO-FEDERFusionNetcombinesgenericobjectdetectionmethodswiththespecializedstrengthofcamouflageobjectdetectiontechniquestoenhancedronedetectioncapabilities.ComprehensiveevaluationsofYOLO-FEDERFusionNetshowtheefficiencyoftheproposedmodelanddemonstratesubstantialim-provementsinbothreducingmisseddetectionsandfalsealarms.IndexTerms—Dronedetection,camouflageobjectdetection,featurefusion,syntheticdata1.INTRODUCTIONRobustdronedetectionsystemsplayavitalroleinenhancingse-curitysystems,protectingprivacyandensuringregulatorycompli-ance[1].Leveragingadvancedcomputervisiontechniques,image-baseddronedetectionestablishesaproactivemechanismtoanalyzevisualdata,facilitatingearlythreatdetection,andenablingeffec-tivemitigationmeasures.Thewidespreadadoptionofimage-baseddetectiontechniquesisprimarilydrivenbythecost-effectivenessofcamerasensors,theirbroadavailability,andtheirseamlessintegra-tionintoestablishedsecuritysystems[2].Inthefieldofdronedetection,theprocessingofacquiredimagedatatypicallyreliesontheapplicationofgenericobjectdetec-tionmodels(e.g.,YOLOv5[3]invariousnetworkconfigurations).Thesemodelsenjoywidespreadpopularityduetotheiradeptnessinbalancingreal-timeprocessingspeedandprecision.Further-more,genericobjectdetectionmodelsexhibitnotableeffectivenessindetectingdronesagainsthomogeneousbackgrounds(e.g.,clearbluesky),orinscenarioswheredronesdistinctlycontrastwiththeirsurroundings[4].However,theirperformancetendstodeclineconsiderablyinscenarioswheredronesoperateagainstcomplexandhighlytexturedbackgrounds[4,5].Ourpreviousinvestigationsspecificallyemphasizedthesubstantialchallengeinherentindetect-ingdronesamidstorincloseproximitytotrees.Theheterogeneousbackgroundcomposition,combinedwithdifficultlightingcondi-tionsandthesimilaritybetweentreebranchesanddronerotorarms,facilitatesaseamlessintegrationofdronesintotheirsurroundings.∗CorrespondingAuthor,E-mail:tamara.lenhard@dlr.deFig.1.VisualcomparisonbetweenYOLO-FEDERFusionNet(redboundingboxes)andYOLOv5l(blueboundingbox),showcasingeveryfifthimageframe.YOLO-FEDERFusionNetconsistentlyde-tectsthedroneacrossallsixframes,whereasYOLOv5lonlyidenti-fiesitinthelastone.Theresultingcamouflageeffectseverelyimpedesthecapacityofgenericobjectdetectionsystemstoaccuratelyidentifyanddelin-eatetheboundariesofdrones,underminingtheoveralldetectionquality[5].Thephenomenonofcamouflageeffectsextendsbeyonddronedetection.Forinstance,itconstitutesaconsiderablechallengeinaccuratelydetectinganimalswithintheirnaturalhabitats,foster-ingthedevelopmentofdiversecamouflageobjectdetection(COD)techniques[6].WhileCODtechniqueshavedemonstratedefficiencyinanimaldetection,theirdirecttranspositiontodronedetectionisunexplored.Therefore,ourstudyaimstoassesstheviabilityofleveragingin-sightsfromCODmethodstoenhancethereliabilityofgenericdronedetectors,especiallyinscenarioswheretheyencounterlim-itations(cf.Figure1).WeintroduceYOLO-FEDERFusionNet,anoveldeeplearning(DL)architecturethatcombinesthestrengthsofgenericobjectdetectionwiththespecializedcapabilitiesofCOD.ThenewlydevelopedneckstructureofYOLO-FEDERFusionNetenablesseamlessinformationfusionandprioritizationofessen-tialfeatures.Furthermore,weprovideanexaminationofYOLO-FEDERFusionNetacrossdiversereal-worlddatasets.Thisalsoentailsacomparativeanalysisagainstestablisheddronedetectiontechniques,offeringarobustevaluationoftheeffectivenessandperformanceenhancementsachievedbyourapproach.Additionally,weintroduceasimpletechniquesforfalsenegativemitigationinimagesequences.

Authorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2300

Giventhelimitedavailabilityofannotatedreal-worlddatafordronedetectionandtheproblem-specificnatureofexistingdatasets,ourmethodstrategicallyincorporatessyntheticdata–aprevalentpracticetomitigatethisscarcity[7–9].Nevertheless,thereisstilladiscrepancybetweensyntheticallygeneratedandmanuallylabeledreal-worlddata.Consequently,ourstudyalsoaddressesthegapbe-tweensimulatedscenariosandreal-worldconditions,especiallyfo-cusingontheinherentbiasinducedbymanuallabelingprocedures.Theremainderofthepaperisstructuredasfollows:Section2presentsareviewofthecurrentstate-of-the-art,followedbyadetailedexpositionoftheproposeddronedetectionframework(Section3).Section4describestheexperimentalsetupincludingdatasets,evaluationmetrics,andimplementationdetails.TheresultsarepresentedinSection5.InSection6,wedrawconclusions.2.RELATEDWORKInthefollowing,wediscussthelatestadvancementsinimage-baseddronedetection,focusingontheireffectivenessinchallengingenvi-ronments.Additionally,weaddresstheprincipleideaofCODalongwithprevalentCODtechniques.Furthermore,weexploreessentialresearchandconceptsrelatedtogeneratingsyntheticdata,address-ingtheinherentdiscrepanciesbetweensimulatedandreal-worldsce-nariosinthecontextofdronedetection.DroneDetection.Developingpreciseandreliabledronedetectionsystemsisamultidimensionalchallenge,accommodatingvariousinterpretations,sub-problemsandstrategicdirections.Withinthedomainofimage-baseddronedetection,considerableemphasisisdirectedtowardsthedetectionofsmalldrones[10,11]andaccu-ratelydiscerningthemfromotheraerialentities,suchasbirds[10].Anotabledeficiencypersistsinmethodologiesspecificallytailoredtotheenhancementofdronedetectionamidstcomplexorhighlytexturedbackgrounds[5,10].Recentstrategiesaddressingthischal-lengecommonlyentailtherefinementofdistinctmoduleswithinestablishedobjectdetectionframeworks,suchasYOLOv5[10].Forinstance,Lvetal.[10]introduceSAG-YOLOv5s–anadaptedsmallerversionofYOLOv5s(denotingthesmallvariantwithintheYOLOv5series),specificallyoptimizedforintricateenviron-ments.TheirmethodintegratesSimAMattention[12]andGhostmodules[13]intoYOLOv5s’bottleneckstructure,toelevatedronetargetextractionandrefinebackgroundsuppressionduringfeatureanalysis.Concurrently,alternativemethodologiesseektosimplifythecomplexityofnaturalenvironmentsbydividingtheprocessintotwodistinctstages.Typically,thisinvolveseliminatingback-groundelements[4]andextractingmovingobjects[14],followedbyaclassification.Furthermore,somemethodologiescontemplatecamera-baseddronedetectioneitherasapreliminarystageinatrackingprocedureoranintegralpartofamulti-sensorsystem[15].Thisstrategicconsiderationaimstooffsetpotentialshortcomingsofapurelycamera-basedsystem,augmentingitsrobustnessespeciallyincomplexenvironments.However,thechallengeofcamouflageeffectsinducedbynaturalelementsliketreesremainsunaddressedincurrentdronedetectionstrategies.CamouflageObjectDetection.Thedevelopmentofspecializedmethodologiesdedicatedtothedetectionofcamouflageobjectsisanemergingfieldofresearch.Camouflageobjectdetection(COD)embodiesaclass-independentdetectiontask[6],particularlypreva-lentinthedomainofanimaldetection.Itsobjectiveisthepreciseidentificationofobjectsthatcloselyreplicatetheinherentcharac-teristicsoftheirsurrounding,minimizingtheirvisualcontrastanddistinctiveness.ThemajorityofCODtechniquesaddressthechal-lengesposedbyintrinsicsimilarityandedgedisruptionthroughem-ulationofthehumanvisualsystem[6,16].Onlyasmallselectionofapproachesseekstocompensateforperceptionlimitationsbydis-assemblingthecamouflagescenarioandemphasizingsubtledistin-guishingfeatures[17].Apromisingmodelbelongingtothesecondalgorithmiccategoryisthefeaturedecompositionandedgerecon-struction(FEDER)modelbyHeetal.[17].Simulation-RealityGap.LeveragingsyntheticdataisapopularapproachfortrainingDLmodelsindronedetection[5,7–9]andotherapplicationdomains[18]duetothehighexpensestiedtoac-quiringreal-worlddata.Techniqueslikedomainrandomization[8]orgameengine-basedsimulations[19]facilitatethegenerationofextensive,domain-specificdatasets.Thesemethodsdemonstratecost-effectivenessthroughautomatedlabelingprocesses,ensuringpreciseannotations,unlikemanuallabelingtechniques.Further-more,theyenablethecircumventionofreal-worldconstraints(e.g.,privacyregulations),fosteringdatasetdiversification.However,transferringdetectionmodelsexclusivelytrainedonsyntheticdatatoreal-worldapplicationsfrequentlyleadstoperformancedegrada-tion,attributedtothesimulation-realitygap.Thegap’sseverityiscloselylinkedtothequalityofbothsyntheticandreal-worlddata,andistypicallyassessedthroughdiversequalitymeasuressuchasmAPacrossvariousintersectionoverunion(IoU)thresholds[7,8].Twoprimarystrategiesfornarrowingthisgapincludefine-tuningwithreal-worlddata[8]andmixed-datatraining[14].3.FRAMEWORKGiventhecomplexityinherenttodronedetection,theproposedYOLO-FEDERFusionNetstrategicallycombinesthebenefitsofgenericobjectdetectionwiththespecificstrengthsofCODalgo-rithms.Themodelreliesontwoessentialcomponentsforfea-tureextraction:thewell-establishedYOLOv5lbackbonearchitec-ture[3]andthespecializedcamouflageobjectdetectorFEDER[17].YOLOv5lreferstothelargermodelconfigurationwithintheYOLOv5series.Asbothalgorithmsyieldcomplementaryresults,theYOLOv5lbackboneandtheFEDERalgorithmoperateasanen-semblesystemtoextractsignificantfeatures.ThisinvolvesparallelprocessinganRGBimageX∈RW×H×3withW=H,bybothcomponents(cf.Figure2).Theinformationfrombothcomponentsisfusedatfeaturelevelwithinthenetwork’sneck,whosearchitec-turaldesignisinspiredbyYOLOv5l[3].Thefeaturemapsissuedbytheneckareprocessedwithinthenetwork’sheadtogeneratepre-dictionsforobjectsacrossthreedistinctsizes.Detaileddescriptionsofallnetworkcomponentsarepresentedinthefollowingsections.YOLOv5lBackbone.TheemployedYOLOv5lbackbone[3]re-liesonCSPDarkNet53,whichincorporatesDarkNet-53[20]incon-junctionwithanadvancedCSPNetstrategy[21].ThefoundationalarchitecturefeaturesasequentialarrangementofmultipleCBSmod-ules(consistingofconvolutional,batchnormalization,andSiLUac-tivationlayers)andC3modules(comprisingaCSPbottleneckwiththreeconvolutionallayers).Aspatialpyramidpoolingfusion(SPPF)module[22]completesthebackbonestructure(seeFigure2,bottomleft).FEDERBackbone.Thefeaturedecompositionandedgere-construction(FEDER)modelintroducedbyHeetal.[17]con-sistsofthreemaincomponents:acamouflagedfeatureencoderAuthorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2301

YOLOv5l BackboneNeck80×80×256640×640×3InputFinal OutputC3 + CBAMConcat+ CAMFEDERCBS + C3CBS + C3 + SPPFCBS + CBS + C3CBS + C320×20×102440×40×51280×80×25620×20×51240×40×256CBSC3 + CBAMCBS40×40×512Concat+ CAM20×20×1024C3 + CBAMConcat+ CAMCBSUpsampleC3 + CBAMConcat+ CAMUpsampleCBSYOLOv5l DetectionHeadCFEDeep Wavelet-like DecompositionSED640×640×1MapSegmentationBinaryFig.2.OverviewofYOLO-FEDERFusionNet.Keycomponentsfusingandprocessinginformationfrombothbackbonesarehighlightedinred.Layersareabbreviatedasfollows:CFE(camouflagefeatureencoder),CAM(channelattentionmodule),CBS(convolution+batchnormalization+SiLUactivation),CBAM(convolutionalblockattentionmodule),C3(CSPbottleneckwiththreeconvolutionallayers),SED(segmentation-orientededge-assisteddecoder),SPPF(spatialpyramidpoolingfusion).ThevisualizationisinspiredbytheillustrationoftheYOLOv5larchitecturein[3].(CFE),adeepwavelet-likedecomposition(DWD)module,andasegmentation-orientededge-assisteddecoder(SED)(seeFigure2,topleft).TheCFEleveragesaRes2Net50[23],incombinationwithR-Net[6],togenerateaseriesoffeaturemapsgivenaninputimageX∈RW×H×3,withW=H.Thesefeaturemapsserveasinputsforanefficientatrousspatialpyramidpooling(e-ASPP)module[24]andtheDWD.AsdiscriminativeattributesinCODprimarilyresideinhigh-frequency(HF)andlow-frequency(LF)components[25]–suchastextureandedges(HF),aswellascolorandillumination(LF)–thefeaturemapswithintheDWDmodulearepartitionedintodistinctHFandLFparts.Thepartitioningprocessinvolvesem-ployinglearnableHFandLFfilters,coupledwithadaptivewaveletdistillation[26]forupdatingthecoefficients.Furthermore,theDWDleveragesHFandLFattentionmodules,alongsideguidance-basedfeatureaggregation,tosystematicallyextractdiscriminativeinformationfromthedecomposedfeaturesandfusethisinformationinameaningfulway.ThefeaturesderivedfromtheDWDandthee-ASPParedecodedviatheSED.WithintheSED,areversiblere-calibrationsegmentation(RRS)moduleandanordinarydifferentialequation(ODE)-inspirededgereconstruction(OER)moduleareemployedforsophisticatedfeatureprocessingandauxiliaryedgereconstruction.ThefinaloutputgeneratedbytheFEDERbackbonecomprisesabinarysegmentationmapOS∈RW×H×1andanedgepredictionmapOE∈RW×H×1withW=H.InYOLO-FEDERFusionNet,onlythesegmentationmapisfurtherprocessed.FormoredetailsonFEDER,referto[17].Neck.TheneckofYOLO-FEDERFusionNetisspecificallyde-signedtounifyinformationfrombothbackbonesacrossdiverselay-ers(cf.Figure2).Drawinginspirationfromthefoundationalar-chitectureofYOLOv5l[3],theneck’sarchitecturaldesignfeaturesCBS,C3,andupsamplingmodules(akintoYOLOv5l).Inaddition,modifiedconcatenationlayersareincorporatedtofacilitatethein-tegrationofoutputsfromtheFEDERbackbone(cf.Figure2,redconnections),effectivelycomplementinginformationgatheredfromprecedinglayers(cf.Figure2,grayconnections).Furthermore,at-tentionmechanismsarestrategicallyembeddedatmultiplepositionswithinthenetwork’sneck(cf.Figure2,redcomponents)toen-abletheprioritizationofsignificantfeatures.Attentionmechanismsfrequentlyonlyconcentrateoneitherspatialorchannel-relatedfea-turerelationships(cf.[27]).Awidelyadoptedattentionmechanism,combiningspatialandchannel-wiseattention,istheconvolutionalblockattentionmodule(CBAM)introducedbyWooetal.[28].InspiredbyWooetal.’sRes50+CBAMmodel[28],whereCBAMwasembeddedwithintheresidualblocks,weintegratedthismoduleintoourproposednetworkarchitectureinasimilarway.Pre-cisely,itislocatedwithintheCSPbottleneckoftheC3module(cf.Figure2,C3+CBAM).ConsideringanintermediatefeaturemapF∈RW×H×CderivedfromaprecedingCBSmodule(cf.Fig-ure3),theoverallattentionprocessinitiatedbyCBAMcanbede-scribedasfollows:F′=MS(MC(F)⊗F)⊗(MC(F)⊗F).Here,MC(F)∈R1×1×Crepresentstheone-dimensionalchan-nelattentionmap,MS(F)∈RW×H×1thetwo-dimensionalspa-tialattentionmap,⊗denotestheelement-wisemultiplication,andF′∈RW×H×Csignifiestherefinedfeaturemap.Notethatduringmultiplication,MC(F)isreplicatedalongthespatialdimensions,whileMS(F)isduplicatedalongthechanneldimension[28].TheintegrationofCBAMaimstodirectthemodel’sattentiontowardsrelevantareasandoptimizeitsfocus.Additionally,weimplementedachannel-wiseattentionmechanism(akintotheonefeaturedinCBAM,cf.[28]fordetails)followingtheconcatenationofinfor-mationacrossdiverselayers.Forinstance,whenlinkinganinterme-diatefeaturemapF∈RW×H×CobtainedbytheYOLOv5lback-bonewiththebinarysegmentationmapOS∈RW×H×1derivedAuthorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2302

CBSC3 Module + CBAM BottleneckInput FeatureCBSBottleneckOutput FeatureCBAMCBSConcatCBSFig.3.IntegrationofCBAMintothebottleneckoftheC3module,locatedbydefaultintheneckandheadofYOLOv5l.Modifiedpartsarehighlightedinred.fromFEDER,theinstantiatedattentionmechanismcanbedescribedasfollows:F′=MC(Concat(F,OS))⊗Concat(F,OS)whereMC(F)isonceagainreplicatedalongthespatialdimensions.Thismechanismaimstoaccountforinter-dependenciesandrelation-shipsamongdifferentchannelswithinafeaturemap.Therefore,itisparticularbeneficialwhenconsolidatingdatafrommultiplesourcesviaconcatenation,facilitatingtheselectionandprioritizationofthemostrelevantdetailsfromeachsource.Head.TheheadoftheproposednetworkreplicatesthedesignofthestandardYOLOv5lhead.Itsprimaryfunctioninvolvespredict-ingobjectsacrossthreedistinctsizes(small,medium,andlarge).4.EXPERIMENTALSETUPToassesstheproposedframework,weemploythefollowingexperi-mentalsetup,incorporatingdiversedatasetsandevaluationmetrics.4.1.DatasetsConsideringthescarcityofaccessibledatainthecontextofdronedetection,weutilizeself-capturedreal-worlddatasourcedfromapotentialapplicationsiteforevaluation.Concurrently,weleveragesyntheticallygenerateddata,derivedfromphysically-realisticsim-ulations,toeffectivelytraintheproposeddetectionmodel.Table1providesanoverviewofthedatasetsemployedinthisstudy,withdetailsdiscussedbelow.Bothrealandsyntheticallygenerateddatahavealreadybeenincludedinourpriorwork[5].Real-WorldData.Toacquirereal-worlddata,weemployafixedBasleracA200-165ccamerasystemfirmlystationedontheground.Thesystemisequippedwithduallenses(25mmand8mm),enablingthecaptureoftwodistinctfieldofviewsfromeachvantagepoint.Theselectedrecordingenvironmentreplicatesstructuralandenvi-ronmentalcharacteristicsofapotentialinstallationsiteforadronedetectionsysteminanurbansurveillancesetting(cf.[5]formoredetails).TheoriginalRGBimagesarerecordedataresolutionof2040×1086pixels,leadingtotwodistinctdatasetsR1andR2(cf.Table1).WhilethebackgroundcompositionindatasetR1ischarac-terizedbyanincreasedprevalenceofbuildingstructures,highlytex-turedobjects–morepreciselytrees–formasubstantialpartoftheimagebackgroundsindatasetR2.Thus,R2exhibitsagreaterlevelofcomplexityincomparisontoR1.DataannotationwasperformedviaCVAT,leveragingitsintegratedboundingboxtrackingfeature,followedbyadualmanualverificationprocess.Giventhemodel’sconstraintnecessitatingsquareimages,acoarsecroppingstrategyisTable1.Overviewoftrainingandvalidationdatasets.DatasetTypeImageCountCameraTrainValTestPos.FocalLengthR1real7,5242,5082,50828mmR2real3,8341,2791,278225mmS1synth.10,4463,4833,4835–deployed,contingentuponthepreciselocalizationofthedroneob-jectwithintheimageframe.Subsequently,arandomcroppingtech-niqueisappliedusingdistinctdimensions:640×640(YOLOv5l’sdefaultinputsize)and1080×1080.Thisprocedureyieldstwodif-ferentversionsofeachdataset:onesetfeaturingimagesatares-olutionof640×640pixels,andanothersetcomprisingimagesofsize1080×1080pixels(toamplifytheinformationalcontent).Em-ployingthissystematicapproachensuresthepreservationofcrucialinformationwhileconcurrentlyenhancingdatasetdiversity.Along-sidedroneimagery,alldatasetsalsoincludeapproximately7-8%ofbackgroundimages.SyntheticData.Togeneratesynthetictrainingdata,weemploythegameengine-baseddatagenerationpipelinedetailedin[19].ThepipelineharnessesthefunctionalitiesoftheUnrealEngineversion4.27[29]andMicrosoftAirSim[30],enablingtheefficientextrac-tionofautomaticallylabeledRGBimages.LeveragingtheUrbanCityenvironment[31],weaimtoemulateessentialattributesoftheapplicationscenariodefinedbyR1andR2.Datacollectionisper-formedfromfiveuniquecameraperspectives,employingthreedis-tinctdronemodels(see[5]formoredetails).Alignedwiththechar-acteristicsofR1andR2,syntheticRGBimagesareinitiallycapturedataresolutionof2040×1080pixels(leadingtodatasetS1,cf.Ta-ble1).Subsequently,acroppingprocedureisapplied(inanalogytoR1andR2)toachieveafinalresolutionof640×640pixels.DatasetS1alsoincludesasmallshareofbackgroundimages(7-8%).4.2.EvaluationMetricsEnsuringsecurityagainstunauthorizeddroneintrusionnecessitatespreciseearly-stagedetection.Thus,anexceptionallylowfalsenega-tiverate(FNR)isessentialforareliabledetectionsystem.However,inscenariosinvolvingcontinuousdatastreams(e.g.,commonlyencounteredinsurveillancesettings),detectingadroneineveryframeofthesequenceisnotimperative.Extrapolatingfromadja-centframesenables(toacertainextent)partialinferenceofmisseddetections.Consideringdronedetectionasanintegralcomponentofacomprehensivesecurityframework,thereductionoffalsepos-itivesisalsocrucialforsystemcredibility.Thisisakintoreducingthefalsediscoveryrate(FDR).ComplementingtheevaluationviaFNRandFDR,weincludethemeanaverageprecision(mAP)atanintersectionoverunion(IoU)thresholdof0.5,duetoitswidespreadadoptionaskeyperformanceindicatorforobjectdetectionmodels.Astherequirementforpreciseboundingboxlocalizationcanbealleviatedinourapplicationcontextandmanuallygeneratedanno-tationsexhibitnotablevarianceinquality,wealsoconsidermAPvaluesatanIoUthresholdof0.25.4.3.ImplementationDetailsYOLO-FEDERFusionNetisimplementedinPyTorch,leveragingthefoundationoftheoriginalYOLOv5frameworkprovidedby[3].Authorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2303

Table2.EvaluationandcomparisonofYOLOv5l,YOLOv5lSQ,andYOLO-FEDERFusionNetondatasetR1.ModelImgSizemAPFNRFDR@0.25@0.5YOLOv5l2040×10860.5720.5510.4630.500YOLOv5l640×6400.4330.4010.6010.311YOLOv5lSQ0.2090.1910.9130.033FusionNet(Ours)0.7290.6690.3720.114YOLOv5l1080×10800.5680.5480.4570.261YOLOv5lSQ0.4540.3800.9320.078FusionNet(Ours)0.7080.6360.4490.066Table3.EvaluationandcomparisonofYOLOv5l,YOLOv5lSQ,andYOLO-FEDERFusionNetondatasetR2.ModelImgSizemAPFNRFDR@0.25@0.5YOLOv5l2040×10860.5710.4320.7450.290YOLOv5l640×6400.1020.0470.8580.638YOLOv5lSQ0.2520.0780.9550.010FusionNet(Ours)0.6850.2700.4730.029YOLOv5l1080×10800.3960.1960.6980.249YOLOv5lSQ0.3430.1010.9860.058FusionNet(Ours)0.8160.4230.3350.007ItincorporatesaYOLOv5lbackbonepre-trainedontheCOCObenchmarkdataset[32],aswellasaFEDERnetworkinitializedwithCOD10K[6]weights,bothremainingfrozenduringtraining.Themodel’sneckandheadareexclusivelyoptimizedbasedonthesyntheticdatasetS1(cf.Section4.1)usingstochasticgradientdescent(SGD)withaninitiallearningrateof0.01,amomentumof0.937,andaweightdecayof0.005.Inthetrainingphase,wemaintainabatchsizeof32.Weassumesquareinputimagesthatareuniformlyresizedto640×640fortrainingandinference.Wedelib-eratelyavoidletter-boxingorrandomresizingtohandlerectangularimages.Additionally,wetraintwostandardYOLOv5lmodels[3]forcomparativeanalysis,usingthesamehyper-parameterconfigu-rationasforYOLO-FEDERFusionNet.ThefirstmodelistrainedsolelyonthesyntheticdatasetS1initsoriginal,un-croppedversion,asinourpriorwork[5].Thesecondmodel(YOLOv5lSQ)istrainedonthecroppedversionofS1(cf.Section4.1).Inbothscenarios,nolayersarefrozenduringthetrainingprocess.AllexperimentsareconductedonasingleNVIDIAQuadroRTX-8000GPU.5.RESULTSInthissection,wepresenttheevaluationresultsofourproposedframework,providinganexaminationofitsperformanceonreal-worlddata,apost-processingstrategyforlabelingbiasesmitigation,andanassessmentofitseffectivenessinanalarmscenario.5.1.PerformanceonReal-WorldDataExaminingtheperformanceofYOLO-FEDERFusionNetonreal-worlddatasetsR1andR2(withvaryingcutoutsizes,cf.Section4.1)revealspromisingresults,particularlyincontrasttotheoriginalYOLOv5lmodeltrainedandassessedon2040×1086images[5].Specifically,YOLO-FEDERFusionNetexhibitsasignificantde-clineinFDRof77.2%(from0.5to0.114)and86.8%(from0.5to0.066)fordatasetR1(cf.Table2).Additionally,anexceptionalFig.4.VisualcomparisonofthemanuallylabeledGTboundingboxes(blue)andtheboundingboxespredictedbyYOLO-FEDERFusionNet(red).WhiletheGTboxesprovideamoregenerousen-capsulationofthedrone,thepredictedboundingboxesdemonstrateasuperiorlevelofaccuracy.FDRreductionexceeding90.0%isobservedforR2acrossbothimagesizes,withvaluesdecreasingfrom0.29to0.029and0.007(cf.Table3).Furthermore,thereisalsoadistinctreductioninFNRs.ThedirectcomparisonbetweenYOLOv5l–trainedonun-croppedimagesofS1andevaluatedonun-croppedimagesofR1–andYOLO-FEDERFusionNetrevealsavarianceinFNRsof0.091and0.014,respectively.ThisdisparityisfurtherhighlightedwhencontrastingtheevaluationresultsofYOLO-FEDERFusionNetwiththoseofYOLOv5loncroppedversionsofR1(cf.Table2).Asub-stantialdiscrepancyinFNRsbecomesevenmoreevidentfordatasetR2,wherehighlytexturedobjectsformasignificantportionoftheimages’background(cf.Figure1).WhileYOLOv5l,evaluatedontheoriginal-sizedR2images(cf.2040×1086,Table3),exhibitsaFNRof0.745,YOLO-FEDERFusionNetsignificantlyreducesthisrate.Specifically,whenevaluatedon1080×1080imagecutoutsofR2,theFNRdiminishestolessthanhalfofitsoriginalmagni-tude.ThisobservedtrendremainsconsistentwhencontrastedwithYOLOv5lSQ,trainedsimilarlytoYOLO-FEDERFusionNetonacroppedversionofS1.However,owingtoitsconsistentlyhighFNRsandFDRsclosetozero(cf.Tables2and3),YOLOv5lSQdemonstratesageneralinefficiencyinthepresentcontextofdronedetection.Onthecontrary,thishighlightstheperformanceadvan-tagesattainedthroughtheintegrationofYOLOv5landFEDERwithinYOLO-FEDERFusionNet.AnalyzingthemAPvaluesatanIoUthresholdof0.5revealsdis-tincttrends.WithindatasetR1,thereisasignificantimprovementinmAP,risingfrom0.559(cf.YOLOv5l,2040×1086,Table2)to0.636and0.669uponimplementingYOLO-FEDERFusion-Net.Conversely,amarginaldeclineinmAPvaluesisevidentwithindatasetR2(cf.Table3).UponcloserexaminationofthemAPvaluesatanIoUthresholdof0.25,YOLO-FEDERFusionNetdemonstratessuperiorperformancewhencomparedtoYOLOv5l.Specifically,YOLO-FEDERFusionNetexhibitsmAPvaluesbetween0.685and0.816,whileYOLOv5lsolelyregistersvaluesbelow0.572.5.2.LabelingBiasDespitetheprecisedronelocalizationcapabilitiesofYOLO-FEDERFusionNet(seeFigure4),adeeperanalysiscomparingpredictedAuthorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2304

Table4.Categorizationofpredictedboundingboxdimensionsintodistinctgroups,includingsize-dependentscalingfactorsforbound-ingboxcorrectionagainstlabelingbiases.CategoryWidthRatioHeightRatioScalingFactorsminmaxminmaxλwλhExtraSmall0.0000.0340.0000.0140.01550.0110Small0.0340.0590.0140.0270.01070.0055Medium0.0590.0940.0270.0440.00710.0020Large0.0940.1440.0440.0720.00440.0014ExtraLarge0.1441.0000.0721.0000.00220.0011Table5.MeanaverageprecisionofYOLO-FEDERFusionNetondatasetsR1andR2consideringmanuallabelingbiases.DatasetImgSizeLabelingBiasmAPIncludedType@0.25@0.5R1640×640✗–0.7290.669✓fixed0.7290.700✓variable0.7290.710R11080×1080✗–0.7080.636✓fixed0.7080.666✓variable0.7080.681R2640×640✗–0.6850.270✓fixed0.7140.317✓variable0.7200.416R21080×1080✗–0.8160.423✓fixed0.8270.472✓variable0.8360.701boundingboxeswiththegroundtruth(GT)revealsadiscrepancyintheirspatialoverlap.Unlikesynthetictrainingdata,whichfea-turespixel-preciselabeling,themanualannotationsofR1andR2seemtoincludemorepixelsthannecessarytoaccuratelylocalizeadrone.Consequently,theytendtocoveraslightlylargerareatoensurecomprehensiveobjectencapsulationwithhighcertainty.Forinstance,72.71%ofthepredictedboundingboxesofR1(640×640)areentirelycontainedwithintheGT.FordatasetR2(1080×1080),it’s63.45%.However,thisbiasinmanuallabelingandtheresultinglocalizationnoise[33]significantlycompromisedetectionquality,leadingtoaninferiormodelperformanceintermsofmAP(espe-ciallyinscenariosinvolvingbothsyntheticandreal-worlddata).IdentifyingobjectdetectionerrorsbeyondmAP,assessingtheirimpactondetectionquality,andcorrectingpredictionsareimpor-tanttopicsofcurrentresearch(cf.e.g.,[33,34]).Hence,wepro-posetheintegrationofapost-processingstrategytocompensatefordeviationsstemmingfrommanuallabeling.Akeyadvantageofthisstrategyliesinitscapacitytoobviatethenecessityformodi-fyingexistingdatasetsandundergoingre-training.Theprocessen-tailstherefinementofpredictedboundingboxes,characterizedbywidthwandheighth,throughaformalbiascompensationapproach:w′=w+λw(w·h)andh′=h+λh(w·h),wherew′andh′signifytheadjustedboundingboxwidthandheight.Thescalingfactorsλwandλhcanbetailoredindividually.Inourevaluation,weconsideredbothfixedfactors(λw=0.0057andλh=0.0023)andadaptivescalingfactorslinkedtotheobject’ssize(cf.Table4).Notably,λwandλhdiminishwithlargerobjectsizes,assmallerobjectstendtoexhibitmorepronouncedlabelingbiasduetotheintricacyinvolvedTable6.EvolutionofFNRforindividualcamerapositionsofdatasetR2(1080×1080)relativetothesequencesize.CameraPos.SequenceSize(#Frames)1511172127POS10.2690.1530.1180.0870.0530.052POS20.2660.1750.1550.1170.1130.122intheirlabelingprocess.AsillustratedinTable5,accountingformanuallabelingbiasenhancesthemAP,especiallyatanIoUthresholdof0.5.Acon-siderableimprovementcanbeobservedfordatasetR2,suggestinganimpactofbackgroundcomplexityontheextentofthelabelingbias.Thus,addressingthisbiasseemstobeparticularlybeneficialinscenarioscharacterizedbyintricateorhighlytexturedbackgrounds,suchastrees.5.3.DroneDetectioninanAlarmScenarioDronedetectioncanalsobeseenasanintegralcomponentofacom-prehensivesecuritysystem,specificallytargetingtheidentificationofpotentialdronethreatsandthesubsequentactivationofwarn-ingmechanisms.Hence,inferringtheexistenceofadronewithinavideosequencedoesnotnecessarilymandateaframe-by-framedetection.Alternatively,thepresenceofadronecanbeinferredbasedonapartialsequenceofframes,whereitsappearanceinatleastoneframeindicatesitsexistence.Thisstrategyleadstoade-clineofmisseddetections(cf.Table6),albeitattheexpenseofanincreasedinferencetime.6.CONCLUSIONInthiswork,weexploredtheeffectivenessofintegratinggenericob-jectdetectionalgorithmswithCODtechniquesfordronedetectioninenvironmentswithcomplexbackgrounds.WeintroducedYOLO-FEDERFusionNet,anovelDLarchitecture.Alongsidetheintegra-tionofdualbackbones,weimplementedaredesignedneckstructuretoenableseamlessinformationfusionandfacilitatetheprioritiza-tionofessentialfeatures.Wesystematicallyevaluatedtheproposeddetectionmodelonavarietyofrealandsyntheticdatasets,charac-terizedbydifferentcomplexitylevels.OuranalysesdemonstratedsubstantialimprovementsofYOLO-FEDERFusionNetovercon-ventionaldronedetectors,especiallyintermsofFNRsandFDRs.Furthermore,werevealedalabelingbiasoriginatingfrommanuallygeneratedannotationsinreal-worlddata,adverselyaffectingmAPvalues.Addressingthisbiasviapost-processingledtoimprove-mentsw.r.t.mAP.WealsoshowedthatleveraginginformationfrompreviousframesinavideostreamcanfurtherreduceFNRs.Funding:Nofundingwasreceivedforconductingthisstudy.ConflictsofInterest:Theauthorsdeclarenorelevantfinancialornon-financialconflictsofinterest.7.REFERENCES[1]F.-L.Chiper,A.Martian,C.Vladeanu,etal.,“DroneDetectionandDefenseSystems:SurveyandaSoftware-DefinedRadio-BasedSolution,”Sensors,vol.22,no.4,2022.[2]M.Elsayed,M.Reda,A.S.Mashaly,etal.,“ReviewonReal-TimeDroneDetectionBasedonVisualBandElectro-OpticalAuthorized licensed use limited to: UNIVERSIDADE FEDERAL DE GOIAS. Downloaded on August 21,2025 at 13:29:13 UTC from IEEE Xplore.  Restrictions apply.

2305

(EO)Sensor,”in10thInternationalConferenceonIntelligentComputingandInformationSystems,2021,pp.57–65.[3]Ultralytics,“YOLOv5:TheFriendliestAIArchitectureYou’llEverUse,”https://ultralytics.com/yolov5/,ac-cessed:2024-06-12.[4]U.Seidaliyeva,D.Akhmetov,L.Ilipbayeva,etal.,“Real-TimeandAccurateDroneDetectioninaVideowithaStaticBack-ground,”Sensors,vol.20,no.14,2020.[5]T.R.Dieter,A.Weinmann,S.J¨ager,etal.,“QuantifyingtheSimulation–RealityGapforDeepLearning-BasedDroneDe-tection,”Electronics,vol.12,no.10,2023.[6]D.-P.Fan,G.-P.Ji,G.Sun,etal.,“CamouflagedObjectDe-tection,”inIEEE/CVFConferenceonComputerVisionandPatternRecognition,2020,pp.2774–2784.[7]A.Barisic,F.Petric,andS.Bogdan,“Sim2Air-SyntheticAerialDatasetforUAVMonitoring,”IEEERoboticsandAu-tomationLetters,vol.7,no.2,pp.3757–3764,2022.[8]D.Marez,S.Borden,andL.Nans,“UAVDetectionwithaDatasetAugmentedbyDomainRandomization,”inGeospa-tialInformaticsX,2020,vol.11398.[9]C.Symeonidis,C.Anastasiadis,andN.Nikolaidis,“AUAVVideoDataGenerationFrameworkforImprovedRobustnessofUAVDetectionMethods,”inIEEE24thInternationalWork-shoponMultimediaSignalProcessing,2022,pp.1–5.[10]Y.Lv,Z.Ai,M.Chen,etal.,“High-ResolutionDroneDetec-tionBasedonBackgroundDifferenceandSAG-YOLOv5s,”Sensors,vol.22,no.15,2022.[11]H.Liu,K.Fan,Q.Ouyang,etal.,“Real-TimeSmallDronesDetectionBasedonPrunedYOLOv4,”Sensors,vol.21,no.10,2021.[12]L.Yang,R.-Y.Zhang,L.Li,etal.,“SimAM:ASimple,Parameter-FreeAttentionModuleforConvolutionalNeuralNetworks,”in38thInternationalConferenceonMachineLearning,2021,vol.139ofProceedingsofMachineLearningResearch,pp.11863–11874.[13]K.Han,Y.Wang,Q.Tian,etal.,“GhostNet:MoreFeaturesFromCheapOperations,”inIEEE/CVFConferenceonCom-puterVisionandPatternRecognition,2020,pp.1577–1586.[14]Y.Chen,P.Aggarwal,J.Choi,etal.,“ADeepLearningAp-proachtoDroneMonitoring,”inAsia-PacificSignalandIn-formationProcessingAssociationAnnualSummitandConfer-ence,2017,pp.686–691.[15]F.Svanstr¨om,F.Alonso-Fernandez,andC.Englund,“DroneDetectionandTrackinginReal-TimebyFusionofDifferentSensingModalities,”Drones,vol.6,no.11,2022.[16]Q.Jia,S.Yao,Y.Liu,etal.,“Segment,MagnifyandRe-iterate:DetectingCamouflagedObjectstheHardWay,”inIEEE/CVFConferenceonComputerVisionandPatternRecog-nition,2022,pp.4713–4722.[17]C.He,K.Li,Y.Zhang,etal.,“CamouflagedObjectDetec-tionwithFeatureDecompositionandEdgeReconstruction,”inIEEE/CVFConferenceonComputerVisionandPatternRecog-nition,2023,pp.22046–22055.[18]H.X.Pham,A.Sarabakha,M.Odnoshyvkin,etal.,“Pencil-Net:Zero-ShotSim-to-RealTransferLearningforRobustGatePerceptioninAutonomousDroneRacing,”IEEERoboticsandAutomationLetters,vol.7,no.4,pp.11847–11854,2022.[19]T.R.Dieter,A.Weinmann,andE.Brucherseifer,“Generat-ingSyntheticDataforDeepLearning-BasedDroneDetection,”AIPConferenceProceedings,vol.2939,no.1,pp.030007,2023.[20]J.RedmonandA.Farhadi,“Yolov3:Anincrementalimprove-ment,”ArXiv,vol.abs/1804.02767,2018.[21]C.-Y.Wang,H.-Y.MarkLiao,Y.-H.Wu,etal.,“CSPNet:ANewBackbonethatcanEnhanceLearningCapabilityofCNN,”inIEEE/CVFConferenceonComputerVisionandPat-ternRecognitionWorkshop,2020,pp.1571–1580.[22]K.He,X.Zhang,S.Ren,etal.,“SpatialPyramidPoolinginDeepConvolutionalNetworksforVisualRecognition,”IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.37,pp.1904–1916,2014.[23]S.Gao,M.-M.Cheng,K.Zhao,etal.,“Res2Net:ANewMulti-ScaleBackboneArchitecture,”IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.43,pp.652–662,2019.[24]L.-C.Chen,G.Papandreou,I.Kokkinos,etal.,“DeepLab:SemanticImageSegmentationwithDeepConvolutionalNets,AtrousConvolution,andFullyConnectedCRFs,”IEEETrans-actionsonPatternAnalysisandMachineIntelligence,vol.40,pp.834–848,2016.[25]M.StevensandS.Merilaita,“AnimalCamouflage:CurrentIssuesandNewPerspectives,”PhilosophicalTransactionsoftheRoyalSocietyB:BiologicalSciences,vol.364,no.1516,pp.423–427,2009.[26]W.Ha,C.Singh,F.Lanusse,etal.,“AdaptiveWaveletDis-tillationfromNeuralNetworksThroughInterpretations,”inNeuralInformationProcessingSystems,2021.[27]M.-H.Guo,T.Xu,J.Liu,etal.,“AttentionMechanismsinComputerVision:ASurvey,”ComputationalVisualMedia,vol.8,pp.331–368,2021.[28]S.Woo,J.Park,J.-Y.Lee,etal.,“CBAM:ConvolutionalBlockAttentionModule,”inEuropeanConferenceonComputerVi-sion,2018,p.3–19.[29]EpicGames,“UnrealEngine,”https://www.unrealengine.com/en-US/,accessed:2024-06-12.[30]MicrosoftResearch,“WelcometoAirSim,”https://microsoft.github.io/AirSim/,accessed:2024-06-12.[31]PolyPixel,“UrbanCity,”https://www.unrealengine.com/marketplace/en-US/product/urban-city,accessed:2024-06-12.[32]T.-Y.Lin,M.Maire,S.J.Belongie,etal.,“MicrosoftCOCO:CommonObjectsinContext,”inEuropeanConferenceonComputerVision,2014.[33]K.Ryoo,Y.Jo,S.Lee,M.Kim,A.Jo,S.H.Kim,S.Kim,andS.Lee,“UniversalNoiseAnnotation:UnveilingtheIm-pactofNoisyannotationonObjectDetection,”ArXiv,vol.abs/2312.13822,2023.[34]D.Bolya,S.Foley,J.Hays,andJ.Hoffman,“TIDE:AGeneralToolboxforIdentifyingObjectDetectionErrors,”inEuropeanConferenceonComputerVision,2020.